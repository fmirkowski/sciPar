{
  "papers": [
    {
      "id": "2411.14429v1",
      "title": "Revisiting the Integration of Convolution and Attention for Vision\n  Backbone",
      "author": "Lei Zhu",
      "coauthors": [
        "Xinjiang Wang",
        "Wayne Zhang",
        "Rynson W. H. Lau"
      ],
      "abstract": "Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\nconsidered alternatives to each other for building vision backbones. Although\nsome works try to integrate both, they apply the two operators simultaneously\nat the finest pixel granularity. With Convs responsible for per-pixel feature\nextraction already, the question is whether we still need to include the heavy\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\nscalability issue w.r.t. the input resolution for vision transformers. To\naddress this important problem, we propose in this work to use MSHAs and Convs\nin parallel \\textbf{at different granularity levels} instead. Specifically, in\neach layer, we use two different ways to represent an image: a fine-grained\nregular grid and a coarse-grained set of semantic slots. We apply different\noperations to these two representations: Convs to the grid for local features,\nand MHSAs to the slots for global features. A pair of fully differentiable soft\nclustering and dispatching modules is introduced to bridge the grid and set\nrepresentations, thus enabling local-global fusion. Through extensive\nexperiments on various vision tasks, we empirically verify the potential of the\nproposed integration scheme, named \\textit{GLMix}: by offloading the burden of\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\nfew (e.g., 64) semantic slots to match the performance of recent\nstate-of-the-art backbones, while being more efficient. Our visualization\nresults also demonstrate that the soft clustering module produces a meaningful\nsemantic grouping effect with only IN1k classification supervision, which may\ninduce better interpretability and inspire new weakly-supervised semantic\nsegmentation approaches. Code will be available at\n\\url{https://github.com/rayleizhu/GLMix}."
    },
    {
      "id": "2301.00170",
      "title": "Democratization of Retail Trading: Can Reddit's WallStreetBets\n  Outperform Investment Bank Analysts?",
      "author": "Tolga Buz",
      "coauthors": [
        "Gerard de Melo"
      ],
      "abstract": "The recent hype around Reddit's WallStreetBets (WSB) community has inspired\nresearch on its impact on our economy and society. Still, one important\nquestion remains: Can WSB's community of anonymous contributors actually\nprovide valuable investment advice and possibly even outperform top financial\ninstitutions? We present a data-driven empirical study of investment\nrecommendations of WSB in comparison to recommendations made by leading\ninvestment banks, based on more than 1.6 million WSB posts published since\n2018. %enriched with stock market data. To this end, we extract and evaluate\ninvestment recommendations from WSB's raw text for all S&P 500 stocks and\ncompare their performance to more than 16,000 analyst recommendations from the\nlargest investment banks. While not all WSB recommendations prove profitable,\nour results show that they achieve average returns that compete with the best\nbanks and outperform them in certain cases. Furthermore, the WSB community has\nbeen better than almost all investment banks at detecting top-performing\nstocks. We conclude that WSB may indeed constitute a freely accessible,\nvaluable source of investment advice.",
      "categories": "q-fin.ST, cs.CY, cs.SI"
    },
    {
      "id": "2411.14425v1",
      "title": "Whack-a-Chip: The Futility of Hardware-Centric Export Controls",
      "author": "Ritwik Gupta",
      "coauthors": [
        "Leah Walker",
        "Andrew W. Reddie"
      ],
      "abstract": "U.S. export controls on semiconductors are widely known to be permeable, with\nthe People's Republic of China (PRC) steadily creating state-of-the-art\nartificial intelligence (AI) models with exfiltrated chips. This paper presents\nthe first concrete, public evidence of how leading PRC AI labs evade and\ncircumvent U.S. export controls. We examine how Chinese companies, notably\nTencent, are not only using chips that are restricted under U.S. export\ncontrols but are also finding ways to circumvent these regulations by using\nsoftware and modeling techniques that maximize less capable hardware.\nSpecifically, we argue that Tencent's ability to power its Hunyuan-Large model\nwith non-export controlled NVIDIA H20s exemplifies broader gains in efficiency\nin machine learning that have eroded the moat that the United States initially\nbuilt via its existing export controls. Finally, we examine the implications of\nthis finding for the future of the United States' export control strategy."
    },
    {
      "id": "2301.00248",
      "title": "Nowcasting Stock Implied Volatility with Twitter",
      "author": "Thomas Dierckx",
      "coauthors": [
        "Jesse Davis",
        "Wim Schoutens"
      ],
      "abstract": "In this study, we predict next-day movements of stock end-of-day implied\nvolatility using random forests. Through an ablation study, we examine the\nusefulness of different sources of predictors and expose the value of attention\nand sentiment features extracted from Twitter. We study the approach on a stock\nuniverse comprised of the 165 most liquid US stocks diversified across the 11\ntraditional market sectors using a sizeable out-of-sample period spanning over\nsix years. In doing so, we uncover that stocks in certain sectors, such as\nConsumer Discretionary, Technology, Real Estate, and Utilities are easier to\npredict than others. Further analysis shows that possible reasons for these\ndiscrepancies might be caused by either excess social media attention or low\noption liquidity. Lastly, we explore how our proposed approach fares throughout\ntime by identifying four underlying market regimes in implied volatility using\nhidden Markov models. We find that most added value is achieved in regimes\nassociated with lower implied volatility, but optimal regimes vary per market\nsector.",
      "categories": "q-fin.CP"
    },
    {
      "id": "2411.14404v1",
      "title": "Resolving Multiple-Dynamic Model Uncertainty in Hypothesis-Driven\n  Belief-MDPs",
      "author": "Ofer Dagan",
      "coauthors": [
        "Tyler Becker",
        "Zachary N. Sunberg"
      ],
      "abstract": "When human operators of cyber-physical systems encounter surprising behavior,\nthey often consider multiple hypotheses that might explain it. In some cases,\ntaking information-gathering actions such as additional measurements or control\ninputs given to the system can help resolve uncertainty and determine the most\naccurate hypothesis. The task of optimizing these actions can be formulated as\na belief-space Markov decision process that we call a hypothesis-driven belief\nMDP. Unfortunately, this problem suffers from the curse of history similar to a\npartially observable Markov decision process (POMDP). To plan in continuous\ndomains, an agent needs to reason over countlessly many possible\naction-observation histories, each resulting in a different belief over the\nunknown state. The problem is exacerbated in the hypothesis-driven context\nbecause each action-observation pair spawns a different belief for each\nhypothesis, leading to additional branching. This paper considers the case in\nwhich each hypothesis corresponds to a different dynamic model in an underlying\nPOMDP. We present a new belief MDP formulation that: (i) enables reasoning over\nmultiple hypotheses, (ii) balances the goals of determining the (most likely)\ncorrect hypothesis and performing well in the underlying POMDP, and (iii) can\nbe solved with sparse tree search."
    },
    {
      "id": "2301.00372",
      "title": "Lying Aversion and Vague Communication: An Experimental Study",
      "author": "Keh-Kuan Sun",
      "coauthors": [
        "Stella Papadokonstantaki"
      ],
      "abstract": "An agent may strategically employ a vague message to mislead an audience's\nbelief about the state of the world, but this may cause the agent to feel guilt\nor negatively impact how the audience perceives the agent. Using a novel\nexperimental design that allows participants to be vague while at the same time\nisolating the internal cost of lying from the social identity cost of appearing\ndishonest, we explore the extent to which these two types of lying costs affect\ncommunication. We find that participants exploit vagueness to be consistent\nwith the truth, while at the same time leveraging the imprecision to their own\nbenefit. More participants use vague messages in treatments where concern with\nsocial identity is relevant. In addition, we find that social identity concerns\nsubstantially affect the length and patterns of vague messages used across the\ntreatments.",
      "categories": "econ.GN, q-fin.EC"
    },
    {
      "id": "2411.14403v1",
      "title": "Landing Trajectory Prediction for UAS Based on Generative Adversarial\n  Network",
      "author": "Jun Xiang",
      "coauthors": [
        "Drake Essick",
        "Luiz Gonzalez Bautista",
        "Junfei Xie",
        "Jun Chen"
      ],
      "abstract": "Models for trajectory prediction are an essential component of many advanced\nair mobility studies. These models help aircraft detect conflict and plan\navoidance maneuvers, which is especially important in Unmanned Aircraft systems\n(UAS) landing management due to the congested airspace near vertiports. In this\npaper, we propose a landing trajectory prediction model for UAS based on\nGenerative Adversarial Network (GAN). The GAN is a prestigious neural network\nthat has been developed for many years. In previous research, GAN has achieved\nmany state-of-the-art results in many generation tasks. The GAN consists of one\nneural network generator and a neural network discriminator. Because of the\nlearning capacity of the neural networks, the generator is capable to\nunderstand the features of the sample trajectory. The generator takes the\nprevious trajectory as input and outputs some random status of a flight.\nAccording to the results of the experiences, the proposed model can output more\naccurate predictions than the baseline method(GMR) in various datasets. To\nevaluate the proposed model, we also create a real UAV landing dataset that\nincludes more than 2600 trajectories of drone control manually by real pilots."
    },
    {
      "id": "2301.00410",
      "title": "Designing organizations for bottom-up task allocation: The role of\n  incentives",
      "author": "Stephan Leitner",
      "coauthors": [],
      "abstract": "In recent years, various decentralized organizational forms have emerged,\nposing a challenge for organizational design. Some design elements, such as\ntask allocation, become emergent properties that cannot be fully controlled\nfrom the top down. The central question that arises in this context is: How can\nbottom-up task allocation be guided towards an effective organizational\nstructure? To address this question, this paper presents a novel agent-based\nmodel of an organization that features bottom-up task allocation that can be\nmotivated by either long-term or short-term orientation on the agents' side.\nThe model also includes an incentive mechanism to guide the bottom-up task\nallocation process and create incentives that range from altruistic to\nindividualistic. Our analysis shows that when bottom-up task allocation is\ndriven by short-term orientation and aligned with the incentive mechanisms, it\nleads to improved organizational performance that surpasses that of\ntraditionally designed organizations. Additionally, we find that the presence\nof altruistic incentive mechanisms within the organization reduces the\nimportance of mirroring in task allocation.",
      "categories": "econ.GN, cs.MA, q-fin.EC, 68U20, 62P20, 90B70, I.6.3; I.6.5; J.4"
    },
    {
      "id": "2411.14351v1",
      "title": "Indiscriminate Disruption of Conditional Inference on Multivariate\n  Gaussians",
      "author": "William N. Caballero",
      "coauthors": [
        "Matthew LaRosa",
        "Alexander Fisher",
        "Vahid Tarokh"
      ],
      "abstract": "The multivariate Gaussian distribution underpins myriad operations-research,\ndecision-analytic, and machine-learning models (e.g., Bayesian optimization,\nGaussian influence diagrams, and variational autoencoders). However, despite\nrecent advances in adversarial machine learning (AML), inference for Gaussian\nmodels in the presence of an adversary is notably understudied. Therefore, we\nconsider a self-interested attacker who wishes to disrupt a decisionmaker's\nconditional inference and subsequent actions by corrupting a set of evidentiary\nvariables. To avoid detection, the attacker also desires the attack to appear\nplausible wherein plausibility is determined by the density of the corrupted\nevidence. We consider white- and grey-box settings such that the attacker has\ncomplete and incomplete knowledge about the decisionmaker's underlying\nmultivariate Gaussian distribution, respectively. Select instances are shown to\nreduce to quadratic and stochastic quadratic programs, and structural\nproperties are derived to inform solution methods. We assess the impact and\nefficacy of these attacks in three examples, including, real estate evaluation,\ninterest rate estimation and signals processing. Each example leverages an\nalternative underlying model, thereby highlighting the attacks' broad\napplicability. Through these applications, we also juxtapose the behavior of\nthe white- and grey-box attacks to understand how uncertainty and structure\naffect attacker behavior."
    },
    {
      "id": "2301.00440",
      "title": "Local Inequities in the Relative Production of and Exposure to Vehicular\n  Air Pollution in Los Angeles",
      "author": "Geoff Boeing",
      "coauthors": [
        "Yougeng Lu",
        "Clemens Pilgram"
      ],
      "abstract": "Vehicular air pollution has created an ongoing air quality and public health\ncrisis. Despite growing knowledge of racial injustice in exposure levels, less\nis known about the relationship between the production of and exposure to such\npollution. This study assesses pollution burden by testing whether local\npopulations' vehicular air pollution exposure is proportional to how much they\ndrive. Through a Los Angeles, California case study we examine how this relates\nto race, ethnicity, and socioeconomic status -- and how these relationships\nvary across the region. We find that, all else equal, tracts whose residents\ndrive less are exposed to more air pollution, as are tracts with a less-White\npopulation. Commuters from majority-White tracts disproportionately drive\nthrough non-White tracts, compared to the inverse. Decades of\nracially-motivated freeway infrastructure planning and residential segregation\nshape today's disparities in who produces vehicular air pollution and who is\nexposed to it, but opportunities exist for urban planning and transport policy\nto mitigate this injustice.",
      "categories": "stat.AP, cs.CY, econ.GN, physics.app-ph, q-fin.EC"
    },
    {
      "id": "2411.14341v1",
      "title": "Logarithmic Neyman Regret for Adaptive Estimation of the Average\n  Treatment Effect",
      "author": "Ojash Neopane",
      "coauthors": [
        "Aaditya Ramdas",
        "Aarti Singh"
      ],
      "abstract": "Estimation of the Average Treatment Effect (ATE) is a core problem in causal\ninference with strong connections to Off-Policy Evaluation in Reinforcement\nLearning. This paper considers the problem of adaptively selecting the\ntreatment allocation probability in order to improve estimation of the ATE. The\nmajority of prior work on adaptive ATE estimation focus on asymptotic\nguarantees, and in turn overlooks important practical considerations such as\nthe difficulty of learning the optimal treatment allocation as well as\nhyper-parameter selection. Existing non-asymptotic methods are limited by poor\nempirical performance and exponential scaling of the Neyman regret with respect\nto problem parameters. In order to address these gaps, we propose and analyze\nthe Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of an\nexisting algorithm with strong asymptotic optimality guarantees, and provide\nfinite sample bounds on its Neyman regret. Our analysis shows that ClipSMT\nachieves exponential improvements in Neyman regret on two fronts: improving the\ndependence on $T$ from $O(\\sqrt{T})$ to $O(\\log T)$, as well as reducing the\nexponential dependence on problem parameters to a polynomial dependence.\nFinally, we conclude with simulations which show the marked improvement of\nClipSMT over existing approaches."
    },
    {
      "id": "2301.00648",
      "title": "Fast Barrier Option Pricing by the COS BEM Method in Heston Model",
      "author": "A. Aimi",
      "coauthors": [
        "C. Guardasoni",
        "L. Ortiz-Gracia",
        "S. Sanfelici"
      ],
      "abstract": "In this work, the Fourier-cosine series (COS) method has been combined with\nthe Boundary Element Method (BEM) for a fast evaluation of barrier option\nprices. After a description of its use in the Black and Scholes (BS) model, the\nfocus of the paper is on the application of the proposed methodology to the\nbarrier option evaluation in the Heston model, where its contribution is\nfundamental to improve computational efficiency and to make BEM appealing among\nFinance practitioners as a valid alternative to Monte Carlo (MC) or other more\ntraditional approaches. An error analysis is provided on the number of terms\nused in the Fourier-cosine series expansion, where the error bound estimation\nis based on the characteristic function of the log-asset price process.",
      "categories": "q-fin.CP, q-fin.PR"
    },
    {
      "id": "2410.24079v2",
      "title": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models",
      "author": "Jinlin Lai",
      "coauthors": [
        "Justin Domke",
        "Daniel Sheldon"
      ],
      "abstract": "Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences."
    },
    {
      "id": "2301.00666",
      "title": "E-commerce users' preferences for delivery options",
      "author": "Yuki Oyama",
      "coauthors": [
        "Daisuke Fukuda",
        "Naoto Imura",
        "Katsuhiro Nishinari"
      ],
      "abstract": "Many e-commerce marketplaces offer their users fast delivery options for free\nto meet the increasing needs of users, imposing an excessive burden on city\nlogistics. Therefore, understanding e-commerce users' preference for delivery\noptions is a key to designing logistics policies. To this end, this study\ndesigns a stated choice survey in which respondents are faced with choice tasks\namong different delivery options and time slots, which was completed by 4,062\nusers from the three major metropolitan areas in Japan. To analyze the data,\nmixed logit models capturing taste heterogeneity as well as flexible\nsubstitution patterns have been estimated. The model estimation results\nindicate that delivery attributes including fee, time, and time slot size are\nsignificant determinants of the delivery option choices. Associations between\nusers' preferences and socio-demographic characteristics, such as age, gender,\nteleworking frequency and the presence of a delivery box, were also suggested.\nMoreover, we analyzed two willingness-to-pay measures for delivery, namely, the\nvalue of delivery time savings (VODT) and the value of time slot shortening\n(VOTS), and applied a non-semiparametric approach to estimate their\ndistributions in a data-oriented manner. Although VODT has a large\nheterogeneity among respondents, the estimated median VODT is 25.6 JPY/day,\nimplying that more than half of the respondents would wait an additional day if\nthe delivery fee were increased by only 26 JPY, that is, they do not\nnecessarily need a fast delivery option but often request it when cheap or\nalmost free. Moreover, VOTS was found to be low, distributed with the median of\n5.0 JPY/hour; that is, users do not highly value the reduction in time slot\nsize in monetary terms. These findings on e-commerce users' preferences can\nhelp in designing levels of service for last-mile delivery to significantly\nimprove its efficiency.",
      "categories": "econ.GN, cs.LG, q-fin.EC"
    },
    {
      "id": "2411.14305v1",
      "title": "Outlier-robust Mean Estimation near the Breakdown Point via\n  Sum-of-Squares",
      "author": "Hongjie Chen",
      "coauthors": [
        "Deepak Narayanan Sridharan",
        "David Steurer"
      ],
      "abstract": "We revisit the problem of estimating the mean of a high-dimensional\ndistribution in the presence of an $\\varepsilon$-fraction of adversarial\noutliers.\n  When $\\varepsilon$ is at most some sufficiently small constant, previous\nworks can achieve optimal error rate efficiently\n\\cite{diakonikolas2018robustly, kothari2018robust}. As $\\varepsilon$ approaches\nthe breakdown point $\\frac{1}{2}$, all previous algorithms incur either\nsub-optimal error rates or exponential running time.\n  In this paper we give a new analysis of the canonical sum-of-squares program\nintroduced in \\cite{kothari2018robust} and show that this program efficiently\nachieves optimal error rate for all $\\varepsilon \\in[0,\\frac{1}{2})$. The key\ningredient for our results is a new identifiability proof for robust mean\nestimation that focuses on the overlap between the distributions instead of\ntheir statistical distance as in previous works. We capture this proof within\nthe sum-of-squares proof system, thus obtaining efficient algorithms using the\nsum-of-squares proofs to algorithms paradigm \\cite{raghavendra2018high}."
    },
    {
      "id": "2301.00680",
      "title": "Large-Scale 3D Printing -- Market Analysis",
      "author": "Razan Abdelazim Idris Alzain",
      "coauthors": [],
      "abstract": "The aim of this research is to get a better understanding of the future of\nlarge-scale 3D printing. By developing the market analysis, it will be clear\nwhether large-scale 3D printing is becoming more of a preferred way of printing\ncustom-made parts for production companies. Companies can then choose whether\nto change their ways, for a more profitable less costly method, or stay on the\nroute they are on. By getting deep into this topic, a new world of technology\nis then being discovered and familiarized. With a mix of theoretical and\npractical relevance, a complete coverage could be made on large-scale 3D\nprinting. This paper could then cover all aspects of this topic, and the reader\ncould then make their own judgment if large-scale 3D printing would be the best\noption.",
      "categories": "econ.GN, q-fin.EC"
    },
    {
      "id": "2202.01694v4",
      "title": "Variational Nearest Neighbor Gaussian Process",
      "author": "Luhuan Wu",
      "coauthors": [
        "Geoff Pleiss",
        "John Cunningham"
      ],
      "abstract": "Variational approximations to Gaussian processes (GPs) typically use a small\nset of inducing points to form a low-rank approximation to the covariance\nmatrix. In this work, we instead exploit a sparse approximation of the\nprecision matrix. We propose variational nearest neighbor Gaussian process\n(VNNGP), which introduces a prior that only retains correlations within $K$\nnearest-neighboring observations, thereby inducing sparse precision structure.\nUsing the variational framework, VNNGP's objective can be factorized over both\nobservations and inducing points, enabling stochastic optimization with a time\ncomplexity of $O(K^3)$. Hence, we can arbitrarily scale the inducing point\nsize, even to the point of putting inducing points at every observed location.\nWe compare VNNGP to other scalable GPs through various experiments, and\ndemonstrate that VNNGP (1) can dramatically outperform low-rank methods, and\n(2) is less prone to overfitting than other nearest neighbor methods."
    },
    {
      "id": "2301.00681",
      "title": "Historical Patterns and Recent Impacts of Chinese Investors in United\n  States Real Estate",
      "author": "Kevin Sun",
      "coauthors": [],
      "abstract": "Since supplanting Canada in 2014, Chinese investors have been the lead\nforeign buyers of U.S. real estate, concentrating their purchases in urban\nareas with higher Chinese populations like California. The reasons for\ninvestment include prestige, freedom from capital confiscation, and safe,\ndiversified opportunities from abroad simply being more lucrative and available\nthan in their home country, where the market is eroding. Interestingly, since\n2019, Chinese investors have sold a net 23.6 billion dollars of U.S. commercial\nreal estate, a stark contrast to past acquisitions between 2013 to 2018 where\nthey were net buyers of almost 52 billion dollars worth of properties. A\nsimilar trend appears in the residential real estate segment too. In both 2017\nand 2018, Chinese buyers purchased over 40,000 U.S. residential properties\nwhich were halved in 2019 and steadily declined to only 6,700 in the past year.\nThis turnaround in Chinese investment can be attributed to a deteriorating\nrelationship between the U.S. and China during the Trump Presidency, financial\ndistress in China, and new Chinese government regulations prohibiting outbound\ninvestments. Additionally, while Chinese investment is a small share of U.S.\nreal estate (~1.5% at its peak), it has outsized impacts on market valuations\nof home prices in U.S. zip codes with higher populations of foreign-born\nChinese, increasing property prices and exacerbating the issue of housing\naffordability in these areas. This paper investigates the rapid growth and\ndecline of Chinese investment in U.S. real estate and its effect on U.S. home\nprices in certain demographics.",
      "categories": "econ.GN, q-fin.EC"
    },
    {
      "id": "2411.14288v1",
      "title": "On the Sample Complexity of One Hidden Layer Networks with Equivariance,\n  Locality and Weight Sharing",
      "author": "Arash Behboodi",
      "coauthors": [
        "Gabriele Cesa"
      ],
      "abstract": "Weight sharing, equivariance, and local filters, as in convolutional neural\nnetworks, are believed to contribute to the sample efficiency of neural\nnetworks. However, it is not clear how each one of these design choices\ncontribute to the generalization error. Through the lens of statistical\nlearning theory, we aim to provide an insight into this question by\ncharacterizing the relative impact of each choice on the sample complexity. We\nobtain lower and upper sample complexity bounds for a class of single hidden\nlayer networks. It is shown that the gain of equivariance is directly\nmanifested in the bound, while getting a similar increase for weight sharing\ndepends on the sharing mechanism. Among our results, we obtain a completely\ndimension-free bound for equivariant networks for a class of pooling\noperations. We show that the bound depends merely on the norm of filters, which\nis tighter than using the spectral norm of the respective matrix. We also\ncharacterize the trade-off in sample complexity between the parametrization of\nfilters in spatial and frequency domains, particularly when spatial filters are\nlocalized as in vanilla convolutional neural networks."
    },
    {
      "id": "2301.00790",
      "title": "Online learning techniques for prediction of temporal tabular datasets\n  with regime changes",
      "author": "Thomas Wong",
      "coauthors": [
        "Mauricio Barahona"
      ],
      "abstract": "The application of deep learning to non-stationary temporal datasets can lead\nto overfitted models that underperform under regime changes. In this work, we\npropose a modular machine learning pipeline for ranking predictions on temporal\npanel datasets which is robust under regime changes. The modularity of the\npipeline allows the use of different models, including Gradient Boosting\nDecision Trees (GBDTs) and Neural Networks, with and without feature\nengineering. We evaluate our framework on financial data for stock portfolio\nprediction, and find that GBDT models with dropout display high performance,\nrobustness and generalisability with reduced complexity and computational cost.\nWe then demonstrate how online learning techniques, which require no retraining\nof models, can be used post-prediction to enhance the results. First, we show\nthat dynamic feature projection improves robustness by reducing drawdown in\nregime changes. Second, we demonstrate that dynamical model ensembling based on\nselection of models with good recent performance leads to improved Sharpe and\nCalmar ratios of out-of-sample predictions. We also evaluate the robustness of\nour pipeline across different data splits and random seeds with good\nreproducibility.",
      "categories": "q-fin.CP, cs.CE, cs.LG"
    },
    {
      "id": "2411.12700v2",
      "title": "Learning multivariate Gaussians with imperfect advice",
      "author": "Arnab Bhattacharyya",
      "coauthors": [
        "Davin Choo",
        "Philips George John",
        "Themis Gouleakis"
      ],
      "abstract": "We revisit the problem of distribution learning within the framework of\nlearning-augmented algorithms. In this setting, we explore the scenario where a\nprobability distribution is provided as potentially inaccurate advice on the\ntrue, unknown distribution. Our objective is to develop learning algorithms\nwhose sample complexity decreases as the quality of the advice improves,\nthereby surpassing standard learning lower bounds when the advice is\nsufficiently accurate.\n  Specifically, we demonstrate that this outcome is achievable for the problem\nof learning a multivariate Gaussian distribution $N(\\boldsymbol{\\mu},\n\\boldsymbol{\\Sigma})$ in the PAC learning setting. Classically, in the\nadvice-free setting, $\\tilde{\\Theta}(d^2/\\varepsilon^2)$ samples are sufficient\nand worst case necessary to learn $d$-dimensional Gaussians up to TV distance\n$\\varepsilon$ with constant probability. When we are additionally given a\nparameter $\\tilde{\\boldsymbol{\\Sigma}}$ as advice, we show that\n$\\tilde{O}(d^{2-\\beta}/\\varepsilon^2)$ samples suffices whenever $\\|\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} \\boldsymbol{\\Sigma}\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} - \\boldsymbol{I_d} \\|_1 \\leq \\varepsilon\nd^{1-\\beta}$ (where $\\|\\cdot\\|_1$ denotes the entrywise $\\ell_1$ norm) for any\n$\\beta > 0$, yielding a polynomial improvement over the advice-free setting."
    },
    {
      "id": "2405.06808",
      "title": "Large Language Model in Financial Regulatory Interpretation",
      "author": "Zhiyu Cao",
      "coauthors": [
        "Zachary Feinstein"
      ],
      "abstract": "This study explores the innovative use of Large Language Models (LLMs) as\nanalytical tools for interpreting complex financial regulations. The primary\nobjective is to design effective prompts that guide LLMs in distilling verbose\nand intricate regulatory texts, such as the Basel III capital requirement\nregulations, into a concise mathematical framework that can be subsequently\ntranslated into actionable code. This novel approach aims to streamline the\nimplementation of regulatory mandates within the financial reporting and risk\nmanagement systems of global banking institutions. A case study was conducted\nto assess the performance of various LLMs, demonstrating that GPT-4 outperforms\nother models in processing and collecting necessary information, as well as\nexecuting mathematical calculations. The case study utilized numerical\nsimulations with asset holdings -- including fixed income, equities, currency\npairs, and commodities -- to demonstrate how LLMs can effectively implement the\nBasel III capital adequacy requirements.\n  Keywords: Large Language Models, Prompt Engineering, LLMs in Finance, Basel\nIII, Minimum Capital Requirements, LLM Ethics",
      "categories": "q-fin.RM, cs.AI, cs.CL"
    },
    {
      "id": "2411.14166v1",
      "title": "SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized\n  Bilevel Optimization",
      "author": "Shuchen Zhu",
      "coauthors": [
        "Boao Kong",
        "Songtao Lu",
        "Xinmeng Huang",
        "Kun Yuan"
      ],
      "abstract": "This paper studies decentralized bilevel optimization, in which multiple\nagents collaborate to solve problems involving nested optimization structures\nwith neighborhood communications. Most existing literature primarily utilizes\ngradient tracking to mitigate the influence of data heterogeneity, without\nexploring other well-known heterogeneity-correction techniques such as EXTRA or\nExact Diffusion. Additionally, these studies often employ identical\ndecentralized strategies for both upper- and lower-level problems, neglecting\nto leverage distinct mechanisms across different levels. To address these\nlimitations, this paper proposes SPARKLE, a unified Single-loop Primal-dual\nAlgoRithm frameworK for decentraLized bilEvel optimization. SPARKLE offers the\nflexibility to incorporate various heterogeneitycorrection strategies into the\nalgorithm. Moreover, SPARKLE allows for different strategies to solve upper-\nand lower-level problems. We present a unified convergence analysis for\nSPARKLE, applicable to all its variants, with state-of-the-art convergence\nrates compared to existing decentralized bilevel algorithms. Our results\nfurther reveal that EXTRA and Exact Diffusion are more suitable for\ndecentralized bilevel optimization, and using mixed strategies in bilevel\nalgorithms brings more benefits than relying solely on gradient tracking."
    },
    {
      "id": "2303.11414",
      "title": "Cost of Implementation of Basel III reforms in Bangladesh -- A Panel\n  data analysis",
      "author": "Dipti Rani Hazra",
      "coauthors": [
        "Md. Shah Naoaj",
        "Mohammed Mahinur Alam",
        "Abdul Kader"
      ],
      "abstract": "Inspired by the recent debate on the macroeconomic implications of the new\nbank regulatory standards known as Basel III, we tried to find out in this\nstudy that the impact of Basel III liquidity and capital requirements in\nBangladesh proposed by Basel Committee on Banking Supervision (BCBS, 2010a). A\nsmall set of macro variables, using a sample of 22 private commercial banks\noperating in Bangladesh for the period of 2010-2014, are used to estimate\nlong-run relationships among the variables. The macroeconomic variables are\nincluded The profitability of banks, GDP, banks' lending to private sector, Net\nStable Funding Ratio, Tier 1 capital Ratio, Interest rate spread, real interest\nrate. The cost is quantified using Driscoll and Kraay panel data models with\nfixed effect. Impact of higher capital and liquidity requirement on Interest\nrate spread and lending to private sector of banks were considered as the cost\nto the economy as a whole whereas impact of higher capital and liquidity\nrequirement on profitability of banks(ROE) was considered as the cost of banks.\nHere it is found that, the interest rate level is positively affected by the\ntighter liquidity and capital requirements which driven toward lessen of the\nprivate sector lending of banks. The return on equity of banks varies\nnegatively with the liquidity and capital. The economic costs are considerably\nbelow the estimated positive benefit that the reform should have by reducing\nthe probability of banking crises and the associated banking losses (BCBS,\n2010b).",
      "categories": "q-fin.RM"
    },
    {
      "id": "2408.03769v2",
      "title": "Nadaraya-Watson kernel smoothing as a random energy model",
      "author": "Jacob A. Zavatone-Veth",
      "coauthors": [
        "Cengiz Pehlevan"
      ],
      "abstract": "Precise asymptotics have revealed many surprises in high-dimensional\nregression. These advances, however, have not extended to perhaps the simplest\nestimator: direct Nadaraya-Watson (NW) kernel smoothing. Here, we describe how\none can use ideas from the analysis of the random energy model (REM) in\nstatistical physics to compute sharp asymptotics for the NW estimator when the\nsample size is exponential in the dimension. As a simple starting point for\ninvestigation, we focus on the case in which one aims to estimate a\nsingle-index target function using a radial basis function kernel on the\nsphere. Our main result is a pointwise asymptotic for the NW predictor, showing\nthat it re-scales the argument of the true link function. Our work provides a\nfirst step towards a detailed understanding of kernel smoothing in high\ndimensions."
    },
    {
      "id": "2109.09043",
      "title": "Composite Likelihood for Stochastic Migration Model with Unobserved\n  Factor",
      "author": "Antoine Djogbenou",
      "coauthors": [
        "Christian Gouri\u00e9roux",
        "Joann Jasiak",
        "Maygol Bandehali"
      ],
      "abstract": "We introduce the conditional Maximum Composite Likelihood (MCL) estimation\nmethod for the stochastic factor ordered Probit model of credit rating\ntransitions of firms. This model is recommended for internal credit risk\nassessment procedures in banks and financial institutions under the Basel III\nregulations. Its exact likelihood function involves a high-dimensional\nintegral, which can be approximated numerically before maximization. However,\nthe estimated migration risk and required capital tend to be sensitive to the\nquality of this approximation, potentially leading to statistical regulatory\narbitrage. The proposed conditional MCL estimator circumvents this problem and\nmaximizes the composite log-likelihood of the factor ordered Probit model. We\npresent three conditional MCL estimators of different complexity and examine\ntheir consistency and asymptotic normality when n and T tend to infinity. The\nperformance of these estimators at finite T is examined and compared with a\ngranularity-based approach in a simulation study. The use of the MCL estimator\nis also illustrated in an empirical application.",
      "categories": "econ.EM"
    },
    {
      "id": "2411.14003v1",
      "title": "Generative Intervention Models for Causal Perturbation Modeling",
      "author": "Nora Schneider",
      "coauthors": [
        "Lars Lorch",
        "Niki Kilbertus",
        "Bernhard Sch\u00f6lkopf",
        "Andreas Krause"
      ],
      "abstract": "We consider the problem of predicting perturbation effects via causal models.\nIn many applications, it is a priori unknown which mechanisms of a system are\nmodified by an external perturbation, even though the features of the\nperturbation are available. For example, in genomics, some properties of a drug\nmay be known, but not their causal effects on the regulatory pathways of cells.\nWe propose a generative intervention model (GIM) that learns to map these\nperturbation features to distributions over atomic interventions in a\njointly-estimated causal model. Contrary to prior approaches, this enables us\nto predict the distribution shifts of unseen perturbation features while\ngaining insights about their mechanistic effects in the underlying\ndata-generating process. On synthetic data and scRNA-seq drug perturbation\ndata, GIMs achieve robust out-of-distribution predictions on par with\nunstructured approaches, while effectively inferring the underlying\nperturbation mechanisms, often better than other causal inference methods."
    },
    {
      "id": "1808.07339",
      "title": "Scenario-based Risk Evaluation",
      "author": "Ruodu Wang",
      "coauthors": [
        "Johanna F. Ziegel"
      ],
      "abstract": "Risk measures such as Expected Shortfall (ES) and Value-at-Risk (VaR) have\nbeen prominent in banking regulation and financial risk management. Motivated\nby practical considerations in the assessment and management of risks,\nincluding tractability, scenario relevance and robustness, we consider\ntheoretical properties of scenario-based risk evaluation. We propose several\nnovel scenario-based risk measures, including various versions of Max-ES and\nMax-VaR, and study their properties. We establish axiomatic characterizations\nof scenario-based risk measures that are comonotonic-additive or coherent and\nan ES-based representation result is obtained. These results provide a\ntheoretical foundation for the recent Basel III & IV market risk calculation\nformulas. We illustrate the theory with financial data examples.",
      "categories": "q-fin.MF, math.PR"
    },
    {
      "id": "2411.14425v1",
      "title": "Whack-a-Chip: The Futility of Hardware-Centric Export Controls",
      "author": "Ritwik Gupta",
      "coauthors": [
        "Leah Walker",
        "Andrew W. Reddie"
      ],
      "abstract": "U.S. export controls on semiconductors are widely known to be permeable, with\nthe People's Republic of China (PRC) steadily creating state-of-the-art\nartificial intelligence (AI) models with exfiltrated chips. This paper presents\nthe first concrete, public evidence of how leading PRC AI labs evade and\ncircumvent U.S. export controls. We examine how Chinese companies, notably\nTencent, are not only using chips that are restricted under U.S. export\ncontrols but are also finding ways to circumvent these regulations by using\nsoftware and modeling techniques that maximize less capable hardware.\nSpecifically, we argue that Tencent's ability to power its Hunyuan-Large model\nwith non-export controlled NVIDIA H20s exemplifies broader gains in efficiency\nin machine learning that have eroded the moat that the United States initially\nbuilt via its existing export controls. Finally, we examine the implications of\nthis finding for the future of the United States' export control strategy."
    },
    {
      "id": "2406.08222v2",
      "title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion",
      "author": "Sha Luo",
      "coauthors": [
        "Sang Jung Kim",
        "Zening Duan",
        "Kaiping Chen"
      ],
      "abstract": "In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them."
    },
    {
      "id": "2411.10718v3",
      "title": "Transforming Teacher Education in Developing Countries: The Role of\n  Generative AI in Bridging Theory and Practice",
      "author": "Matthew Nyaaba",
      "coauthors": [],
      "abstract": "This study examines the transformative potential of Generative AI (GenAI) in\nteacher education within developing countries, focusing on Ghana, where\nchallenges such as limited pedagogical modeling, performance-based assessments,\nand practitioner-expertise gaps hinder progress. GenAI has the capacity to\naddress these issues by supporting content knowledge acquisition, a role that\ncurrently dominates teacher education programs. By taking on this foundational\nrole, GenAI allows teacher educators to redirect their focus to other critical\nareas, including pedagogical modeling, authentic assessments, and fostering\ndigital literacy and critical thinking. These roles are interconnected,\ncreating a ripple effect where pre-service teachers (PSTs) are better equipped\nto enhance K-12 learning outcomes and align education with workforce needs. The\nstudy emphasizes that GenAI's roles are multifaceted, directly addressing\nresistance to change, improving resource accessibility, and supporting teacher\nprofessional development. However, it cautions against misuse, which could\nundermine critical thinking and creativity, essential skills nurtured through\ntraditional teaching methods. To ensure responsible and effective integration,\nthe study advocates a scaffolding approach to GenAI literacy. This includes\neducating PSTs on its supportive role, training them in ethical use and prompt\nengineering, and equipping them to critically assess AI-generated content for\nbiases and validity. The study concludes by recommending empirical research to\nexplore these roles further and develop practical steps for integrating GenAI\ninto teacher education systems responsibly and effectively."
    },
    {
      "id": "2411.01940v2",
      "title": "Systematic Mapping Study on Requirements Engineering for Regulatory\n  Compliance of Software Systems",
      "author": "Oleksandr Kosenkov",
      "coauthors": [
        "Parisa Elahidoost",
        "Tony Gorschek",
        "Jannik Fischbach",
        "Daniel Mendez",
        "Michael Unterkalmsteiner",
        "Davide Fucci",
        "Rahul Mohanani"
      ],
      "abstract": "Context: As the diversity and complexity of regulations affecting\nSoftware-Intensive Products and Services (SIPS) is increasing, software\nengineers need to address the growing regulatory scrutiny. As with any other\nnon-negotiable requirements, SIPS compliance should be addressed early in SIPS\nengineering - i.e., during requirements engineering (RE). Objectives: In the\nconditions of the expanding regulatory landscape, existing research offers\nscattered insights into regulatory compliance of SIPS. This study addresses the\npressing need for a structured overview of the state of the art in software RE\nand its contribution to regulatory compliance of SIPS. Method: We conducted a\nsystematic mapping study to provide an overview of the current state of\nresearch regarding challenges, principles and practices for regulatory\ncompliance of SIPS related to RE. We focused on the role of RE and its\ncontribution to other SIPS lifecycle phases. We retrieved 6914 studies\npublished from 2017 until 2023 from four academic databases, which we filtered\ndown to 280 relevant primary studies. Results: We identified and categorized\nthe RE-related challenges in regulatory compliance of SIPS and their potential\nconnection to six types of principles and practices. We found that about 13.6%\nof the primary studies considered the involvement of both software engineers\nand legal experts. About 20.7% of primary studies considered RE in connection\nto other process areas. Most primary studies focused on a few popular\nregulation fields and application domains. Our results suggest that there can\nbe differences in terms of challenges and involvement of stakeholders across\ndifferent fields of regulation. Conclusion: Our findings highlight the need for\nan in-depth investigation of stakeholders' roles, relationships between process\nareas, and specific challenges for distinct regulatory fields to guide research\nand practice."
    },
    {
      "id": "2411.13990v1",
      "title": "Repository-level Code Translation Benchmark Targeting Rust",
      "author": "Guangsheng Ou",
      "coauthors": [
        "Mingwei Liu",
        "Yuxuan Chen",
        "Xing Peng",
        "Zibin Zheng"
      ],
      "abstract": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
    },
    {
      "id": "2411.14331v1",
      "title": "Data Formats in Analytical DBMSs: Performance Trade-offs and Future\n  Directions",
      "author": "Chunwei Liu",
      "coauthors": [
        "Anna Pavlenko",
        "Matteo Interlandi",
        "Brandon Haynes"
      ],
      "abstract": "This paper evaluates the suitability of Apache Arrow, Parquet, and ORC as\nformats for subsumption in an analytical DBMS. We systematically identify and\nexplore the high-level features that are important to support efficient\nquerying in modern OLAP DBMSs and evaluate the ability of each format to\nsupport these features. We find that each format has trade-offs that make it\nmore or less suitable for use as a format in a DBMS and identify opportunities\nto more holistically co-design a unified in-memory and on-disk data\nrepresentation. Notably, for certain popular machine learning tasks, none of\nthese formats perform optimally, highlighting significant opportunities for\nadvancing format design. Our hope is that this study can be used as a guide for\nsystem developers designing and using these formats, as well as provide the\ncommunity with directions to pursue for improving these common open formats."
    },
    {
      "id": "2411.14330v1",
      "title": "Datalog with First-Class Facts",
      "author": "Thomas Gilray",
      "coauthors": [
        "Arash Sahebolamri",
        "Yihao Sun",
        "Sowmith Kunapaneni",
        "Sidharth Kumar",
        "Kristopher Micinski"
      ],
      "abstract": "Datalog is a popular logic programming language for deductive reasoning tasks\nin a wide array of applications, including business analytics, program\nanalysis, and ontological reasoning. However, Datalog's restriction to flat\nfacts over atomic constants leads to challenges in working with tree-structured\ndata, such as derivation trees or abstract syntax trees. To ameliorate\nDatalog's restrictions, popular extensions of Datalog support features such as\nexistential quantification in rule heads (Datalog$^\\pm$, Datalog$^\\exists$) or\nalgebraic data types (Souffl\\'e). Unfortunately, these are imperfect solutions\nfor reasoning over structured and recursive data types, with general\nexistentials leading to complex implementations requiring unification, and ADTs\nunable to trigger rule evaluation and failing to support efficient indexing.\n  We present DL$^{\\exists!}$, a Datalog with first-class facts, wherein every\nfact is identified with a Skolem term unique to the fact. We show that this\nrestriction offers an attractive price point for Datalog-based reasoning over\ntree-shaped data, demonstrating its application to databases, artificial\nintelligence, and programming languages. We implemented DL$^{\\exists!}$ as a\nsystem \\slog{}, which leverages the uniqueness restriction of DL$^{\\exists!}$\nto enable a communication-avoiding, massively-parallel implementation built on\nMPI. We show that Slog outperforms leading systems (Nemo, Vlog, RDFox, and\nSouffl\\'e) on a variety of benchmarks, with the potential to scale to thousands\nof threads."
    },
    {
      "id": "2408.04197v2",
      "title": "Pairwise Judgment Formulation for Semantic Embedding Model in Web Search",
      "author": "Mengze Hong",
      "coauthors": [
        "Wailing Ng",
        "Zichang Guo",
        "Chen Jason Zhang"
      ],
      "abstract": "Semantic Embedding Model (SEM), a neural network-based Siamese architecture,\nis gaining momentum in information retrieval and natural language processing.\nIn order to train SEM in a supervised fashion for Web search, the search engine\nquery log is typically utilized to automatically formulate pairwise judgments\nas training data. Despite the growing application of semantic embeddings in the\nsearch engine industry, little work has been done on formulating effective\npairwise judgments for training SEM. In this paper, we make the first in-depth\ninvestigation of a wide range of strategies for generating pairwise judgments\nfor SEM. An interesting (perhaps surprising) discovery reveals that the\nconventional pairwise judgment formulation strategy wildly used in the field of\npairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.\nThrough a large-scale empirical study based on query logs and click-through\nactivities from a major commercial search engine, we demonstrate the effective\nstrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,\nClicked > Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked >\nSkipped) in LTR. We conclude with best practices for training SEM and offer\npromising insights for future research."
    },
    {
      "id": "2406.08222v2",
      "title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion",
      "author": "Sha Luo",
      "coauthors": [
        "Sang Jung Kim",
        "Zening Duan",
        "Kaiping Chen"
      ],
      "abstract": "In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them."
    },
    {
      "id": "2411.14374v1",
      "title": "Using Formal Models, Safety Shields and Certified Control to Validate\n  AI-Based Train Systems",
      "author": "Jan Gruteser",
      "coauthors": [
        "Jan Ro\u00dfbach",
        "Fabian Vu",
        "Michael Leuschel"
      ],
      "abstract": "The certification of autonomous systems is an important concern in science\nand industry. The KI-LOK project explores new methods for certifying and safely\nintegrating AI components into autonomous trains. We pursued a two-layered\napproach: (1) ensuring the safety of the steering system by formal analysis\nusing the B method, and (2) improving the reliability of the perception system\nwith a runtime certificate checker. This work links both strategies within a\ndemonstrator that runs simulations on the formal model, controlled by the real\nAI output and the real certificate checker. The demonstrator is integrated into\nthe validation tool ProB. This enables runtime monitoring, runtime\nverification, and statistical validation of formal safety properties using a\nformal B model. Consequently, one can detect and analyse potential\nvulnerabilities and weaknesses of the AI and the certificate checker. We apply\nthese techniques to a signal detection case study and present our findings."
    },
    {
      "id": "2411.14371v1",
      "title": "Synthesising Robust Controllers for Robot Collectives with Recurrent\n  Tasks: A Case Study",
      "author": "Till Schnittka",
      "coauthors": [
        "Mario Gleirscher"
      ],
      "abstract": "When designing correct-by-construction controllers for autonomous\ncollectives, three key challenges are the task specification, the modelling,\nand its use at practical scale. In this paper, we focus on a simple yet useful\nabstraction for high-level controller synthesis for robot collectives with\noptimisation goals (e.g., maximum cleanliness, minimum energy consumption) and\nrecurrence (e.g., re-establish contamination and charge thresholds) and safety\n(e.g., avoid full discharge, mutually exclusive room occupation) constraints.\nDue to technical limitations (related to scalability and using constraints in\nthe synthesis), we simplify our graph-based setting from a stochastic\ntwo-player game into a single-player game on a partially observable Markov\ndecision process (POMDP). Robustness against environmental uncertainty is\nencoded via partial observability. Linear-time correctness properties are\nverified separately after synthesising the POMDP strategy. We contribute\nat-scale guidance on POMDP modelling and controller synthesis for tasked robot\ncollectives exemplified by the scenario of battery-driven robots responsible\nfor cleaning public buildings with utilisation constraints."
    },
    {
      "id": "2411.14368v1",
      "title": "RV4Chatbot: Are Chatbots Allowed to Dream of Electric Sheep?",
      "author": "Andrea Gatti",
      "coauthors": [
        "Viviana Mascardi",
        "Angelo Ferrando"
      ],
      "abstract": "Chatbots have become integral to various application domains, including those\nwith safety-critical considerations. As a result, there is a pressing need for\nmethods that ensure chatbots consistently adhere to expected, safe behaviours.\nIn this paper, we introduce RV4Chatbot, a Runtime Verification framework\ndesigned to monitor deviations in chatbot behaviour. We formalise expected\nbehaviours as interaction protocols between the user and the chatbot. We\npresent the RV4Chatbot design and describe two implementations that instantiate\nit: RV4Rasa, for monitoring chatbots created with the Rasa framework, and\nRV4Dialogflow, for monitoring Dialogflow chatbots. Additionally, we detail\nexperiments conducted in a factory automation scenario using both RV4Rasa and\nRV4Dialogflow."
    },
    {
      "id": "2411.14367v1",
      "title": "ROSMonitoring 2.0: Extending ROS Runtime Verification to Services and\n  Ordered Topics",
      "author": "Maryam Ghaffari Saadat",
      "coauthors": [
        "Angelo Ferrando",
        "Louise A. Dennis",
        "Michael Fisher"
      ],
      "abstract": "Formal verification of robotic applications presents challenges due to their\nhybrid nature and distributed architecture. This paper introduces ROSMonitoring\n2.0, an extension of ROSMonitoring designed to facilitate the monitoring of\nboth topics and services while considering the order in which messages are\npublished and received. The framework has been enhanced to support these novel\nfeatures for ROS1 -- and partially ROS2 environments -- offering improved\nreal-time support, security, scalability, and interoperability. We discuss the\nmodifications made to accommodate these advancements and present results\nobtained from a case study involving the runtime monitoring of specific\ncomponents of a fire-fighting Uncrewed Aerial Vehicle (UAV)."
    },
    {
      "id": "2410.11112v2",
      "title": "Differentiable Weightless Neural Networks",
      "author": "Alan T. L. Bacellar",
      "coauthors": [
        "Zachary Susskind",
        "Mauricio Breternitz Jr.",
        "Eugene John",
        "Lizy K. John",
        "Priscila M. V. Lima",
        "Felipe M. G. Fran\u00e7a"
      ],
      "abstract": "We introduce the Differentiable Weightless Neural Network (DWN), a model\nbased on interconnected lookup tables. Training of DWNs is enabled by a novel\nExtended Finite Difference technique for approximate differentiation of binary\nvalues. We propose Learnable Mapping, Learnable Reduction, and Spectral\nRegularization to further improve the accuracy and efficiency of these models.\nWe evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware\naccelerator, where they demonstrate superior latency, throughput, energy\nefficiency, and model area compared to state-of-the-art solutions, (2) a\nlow-power microcontroller, where they achieve preferable accuracy to XGBoost\nwhile subject to stringent memory constraints, and (3) ultra-low-cost chips,\nwhere they consistently outperform small models in both accuracy and projected\nhardware area. DWNs also compare favorably against leading approaches for\ntabular datasets, with higher average rank. Overall, our work positions DWNs as\na pioneering solution for edge-compatible high-throughput neural networks."
    },
    {
      "id": "2411.14430v1",
      "title": "Stable Flow: Vital Layers for Training-Free Image Editing",
      "author": "Omri Avrahami",
      "coauthors": [
        "Or Patashnik",
        "Ohad Fried",
        "Egor Nemchinov",
        "Kfir Aberman",
        "Dani Lischinski",
        "Daniel Cohen-Or"
      ],
      "abstract": "Diffusion models have revolutionized the field of content synthesis and\nediting. Recent models have replaced the traditional UNet architecture with the\nDiffusion Transformer (DiT), and employed flow-matching for improved training\nand sampling. However, they exhibit limited generation diversity. In this work,\nwe leverage this limitation to perform consistent image edits via selective\ninjection of attention features. The main challenge is that, unlike the\nUNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it\nunclear in which layers to perform the injection. Therefore, we propose an\nautomatic method to identify \"vital layers\" within DiT, crucial for image\nformation, and demonstrate how these layers facilitate a range of controlled\nstable edits, from non-rigid modifications to object addition, using the same\nmechanism. Next, to enable real-image editing, we introduce an improved image\ninversion method for flow models. Finally, we evaluate our approach through\nqualitative and quantitative comparisons, along with a user study, and\ndemonstrate its effectiveness across multiple applications. The project page is\navailable at https://omriavrahami.com/stable-flow"
    },
    {
      "id": "2411.14424v1",
      "title": "Learning Fair Robustness via Domain Mixup",
      "author": "Meiyu Zhong",
      "coauthors": [
        "Ravi Tandon"
      ],
      "abstract": "Adversarial training is one of the predominant techniques for training\nclassifiers that are robust to adversarial attacks. Recent work, however has\nfound that adversarial training, which makes the overall classifier robust, it\ndoes not necessarily provide equal amount of robustness for all classes. In\nthis paper, we propose the use of mixup for the problem of learning fair robust\nclassifiers, which can provide similar robustness across all classes.\nSpecifically, the idea is to mix inputs from the same classes and perform\nadversarial training on mixed up inputs. We present a theoretical analysis of\nthis idea for the case of linear classifiers and show that mixup combined with\nadversarial training can provably reduce the class-wise robustness disparity.\nThis method not only contributes to reducing the disparity in class-wise\nadversarial risk, but also the class-wise natural risk. Complementing our\ntheoretical analysis, we also provide experimental results on both synthetic\ndata and the real world dataset (CIFAR-10), which shows improvement in class\nwise disparities for both natural and adversarial risks."
    },
    {
      "id": "2411.14421v1",
      "title": "From RNNs to Foundation Models: An Empirical Study on Commercial\n  Building Energy Consumption",
      "author": "Shourya Bose",
      "coauthors": [
        "Yijiang Li",
        "Amy Van Sant",
        "Yu Zhang",
        "Kibaek Kim"
      ],
      "abstract": "Accurate short-term energy consumption forecasting for commercial buildings\nis crucial for smart grid operations. While smart meters and deep learning\nmodels enable forecasting using past data from multiple buildings, data\nheterogeneity from diverse buildings can reduce model performance. The impact\nof increasing dataset heterogeneity in time series forecasting, while keeping\nsize and model constant, is understudied. We tackle this issue using the\nComStock dataset, which provides synthetic energy consumption data for U.S.\ncommercial buildings. Two curated subsets, identical in size and region but\ndiffering in building type diversity, are used to assess the performance of\nvarious time series forecasting models, including fine-tuned open-source\nfoundation models (FMs). The results show that dataset heterogeneity and model\narchitecture have a greater impact on post-training forecasting performance\nthan the parameter count. Moreover, despite the higher computational cost,\nfine-tuned FMs demonstrate competitive performance compared to base models\ntrained from scratch."
    },
    {
      "id": "2408.00754v2",
      "title": "Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal\n  Language Model",
      "author": "Benlin Liu",
      "coauthors": [
        "Yuhao Dong",
        "Yiqin Wang",
        "Zixian Ma",
        "Yansong Tang",
        "Luming Tang",
        "Yongming Rao",
        "Wei-Chiu Ma",
        "Ranjay Krishna"
      ],
      "abstract": "Multimodal language models (MLLMs) are increasingly being applied in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Current methods often rely on specialized\narchitectural designs or task-specific fine-tuning to achieve this. We\nintroduce Coarse Correspondences, a simple lightweight method that enhances\nMLLMs' spatial-temporal reasoning with 2D images as input, without modifying\nthe architecture or requiring task-specific fine-tuning. Our method uses a\nlightweight tracking model to identify primary object correspondences between\nframes in a video or across different image viewpoints, and then conveys this\ninformation to MLLMs through visual prompting. We demonstrate that this simple\ntraining-free approach brings substantial gains to GPT4-V/O consistently on\nfour benchmarks that require spatial-temporal reasoning, including +20.5\\%\nimprovement on ScanQA, +9.7\\% on OpenEQA's episodic memory subset, +6.0\\% on\nthe long-form video benchmark EgoSchema, and +11\\% on the R2R navigation\nbenchmark. Additionally, we show that Coarse Correspondences can also enhance\nopen-source MLLMs' spatial reasoning (by +6.9\\% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen\ndatasets such as SQA3D (+3.1\\%). Taken together, we show that Coarse\nCorrespondences effectively and efficiently boosts models' performance on\ndownstream tasks requiring spatial-temporal reasoning."
    },
    {
      "id": "2411.14411v1",
      "title": "Multi-Agent Environments for Vehicle Routing Problems",
      "author": "Ricardo Gama",
      "coauthors": [
        "Daniel Fuertes",
        "Carlos R. del-Blanco",
        "Hugo L. Fernandes"
      ],
      "abstract": "Research on Reinforcement Learning (RL) approaches for discrete optimization\nproblems has increased considerably, extending RL to an area classically\ndominated by Operations Research (OR). Vehicle routing problems are a good\nexample of discrete optimization problems with high practical relevance where\nRL techniques have had considerable success. Despite these advances,\nopen-source development frameworks remain scarce, hampering both the testing of\nalgorithms and the ability to objectively compare results. This ultimately\nslows down progress in the field and limits the exchange of ideas between the\nRL and OR communities.\n  Here we propose a library composed of multi-agent environments that simulates\nclassic vehicle routing problems. The library, built on PyTorch, provides a\nflexible modular architecture design that allows easy customization and\nincorporation of new routing problems. It follows the Agent Environment Cycle\n(\"AEC\") games model and has an intuitive API, enabling rapid adoption and easy\nintegration into existing reinforcement learning frameworks.\n  The library allows for a straightforward use of classical OR benchmark\ninstances in order to narrow the gap between the test beds for algorithm\nbenchmarking used by the RL and OR communities. Additionally, we provide\nbenchmark instance sets for each environment, as well as baseline RL models and\ntraining code."
    },
    {
      "id": "2411.14402v1",
      "title": "Multimodal Autoregressive Pre-training of Large Vision Encoders",
      "author": "Enrico Fini",
      "coauthors": [
        "Mustafa Shukor",
        "Xiujun Li",
        "Philipp Dufter",
        "Michal Klein",
        "David Haldimann",
        "Sai Aitharaju",
        "Victor Guilherme Turrisi da Costa",
        "Louis B\u00e9thune",
        "Zhe Gan",
        "Alexander T Toshev",
        "Marcin Eichner",
        "Moin Nabi",
        "Yinfei Yang",
        "Joshua M. Susskind",
        "Alaaeldin El-Nouby"
      ],
      "abstract": "We introduce a novel method for pre-training of large-scale vision encoders.\nBuilding on recent advancements in autoregressive pre-training of vision\nmodels, we extend this framework to a multimodal setting, i.e., images and\ntext. In this paper, we present AIMV2, a family of generalist vision encoders\ncharacterized by a straightforward pre-training process, scalability, and\nremarkable performance across a range of downstream tasks. This is achieved by\npairing the vision encoder with a multimodal decoder that autoregressively\ngenerates raw image patches and text tokens. Our encoders excel not only in\nmultimodal evaluations but also in vision benchmarks such as localization,\ngrounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5%\naccuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently\noutperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in\nmultimodal image understanding across diverse settings."
    },
    {
      "id": "2411.14401v1",
      "title": "Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding",
      "author": "Yiming Zhang",
      "coauthors": [
        "Zhuokai Zhao",
        "Zhaorun Chen",
        "Zenghui Ding",
        "Xianjun Yang",
        "Yining Sun"
      ],
      "abstract": "Recent advancements in multimodal large language models (MLLMs) have opened\nnew avenues for video understanding. However, achieving high fidelity in\nzero-shot video tasks remains challenging. Traditional video processing methods\nrely heavily on fine-tuning to capture nuanced spatial-temporal details, which\nincurs significant data and computation costs. In contrast, training-free\napproaches, though efficient, often lack robustness in preserving context-rich\nfeatures across complex video content. To this end, we propose DYTO, a novel\ndynamic token merging framework for zero-shot video understanding that\nadaptively optimizes token efficiency while preserving crucial scene details.\nDYTO integrates a hierarchical frame selection and a bipartite token merging\nstrategy to dynamically cluster key frames and selectively compress token\nsequences, striking a balance between computational efficiency with semantic\nrichness. Extensive experiments across multiple benchmarks demonstrate the\neffectiveness of DYTO, achieving superior performance compared to both\nfine-tuned and training-free methods and setting a new state-of-the-art for\nzero-shot video understanding."
    },
    {
      "id": "2411.14390v1",
      "title": "Persistent Homology for Structural Characterization in Disordered\n  Systems",
      "author": "An Wang",
      "coauthors": [
        "Li Zou"
      ],
      "abstract": "We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. Based on this\nframework, we define a non-parametric metric, the Separation Index (SI), which\nnot only outperforms traditional bond-orientational order parameters in phase\nclassification tasks but also establishes a connection between particle\nenvironments and the global phase structure. Our methods provide an effective\nframework for understanding and analyzing the properties of disordered\nmaterials, with broad potential applications in materials science and even\nwider studies of complex systems."
    },
    {
      "id": "2411.14378v1",
      "title": "CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent\n  Diffusion Models with Neural Fields",
      "author": "Xin-Yang Liu",
      "coauthors": [
        "Meet Hemant Parikh",
        "Xiantao Fan",
        "Pan Du",
        "Qing Wang",
        "Yi-Fan Chen",
        "Jian-Xun Wang"
      ],
      "abstract": "Eddy-resolving turbulence simulations require stochastic inflow conditions\nthat accurately replicate the complex, multi-scale structures of turbulence.\nTraditional recycling-based methods rely on computationally expensive precursor\nsimulations, while existing synthetic inflow generators often fail to reproduce\nrealistic coherent structures of turbulence. Recent advances in deep learning\n(DL) have opened new possibilities for inflow turbulence generation, yet many\nDL-based methods rely on deterministic, autoregressive frameworks prone to\nerror accumulation, resulting in poor robustness for long-term predictions. In\nthis work, we present CoNFiLD-inlet, a novel DL-based inflow turbulence\ngenerator that integrates diffusion models with a conditional neural field\n(CNF)-encoded latent space to produce realistic, stochastic inflow turbulence.\nBy parameterizing inflow conditions using Reynolds numbers, CoNFiLD-inlet\ngeneralizes effectively across a wide range of Reynolds numbers ($Re_\\tau$\nbetween $10^3$ and $10^4$) without requiring retraining or parameter tuning.\nComprehensive validation through a priori and a posteriori tests in Direct\nNumerical Simulation (DNS) and Wall-Modeled Large Eddy Simulation (WMLES)\ndemonstrates its high fidelity, robustness, and scalability, positioning it as\nan efficient and versatile solution for inflow turbulence synthesis."
    },
    {
      "id": "2411.06650v2",
      "title": "Quantum Policy Gradient in Reproducing Kernel Hilbert Space",
      "author": "David M. Bossens",
      "coauthors": [
        "Kishor Bharti",
        "Jayne Thompson"
      ],
      "abstract": "Parametrised quantum circuits offer expressive and data-efficient\nrepresentations for machine learning. Due to quantum states residing in a\nhigh-dimensional Hilbert space, parametrised quantum circuits have a natural\ninterpretation in terms of kernel methods. The representation of quantum\ncircuits in terms of quantum kernels has been studied widely in quantum\nsupervised learning, but has been overlooked in the context of quantum\nreinforcement learning. This paper proposes parametric and non-parametric\npolicy gradient and actor-critic algorithms with quantum kernel policies in\nquantum environments. This approach, implemented with both numerical and\nanalytical quantum policy gradient techniques, allows exploiting the many\nadvantages of kernel methods, including available analytic forms for the\ngradient of the policy and tunable expressiveness. The proposed approach is\nsuitable for vector-valued action spaces and each of the formulations\ndemonstrates a quadratic reduction in query complexity compared to their\nclassical counterparts. Two actor-critic algorithms, one based on stochastic\npolicy gradient and one based on deterministic policy gradient (comparable to\nthe popular DDPG algorithm), demonstrate additional query complexity reductions\ncompared to quantum policy gradient algorithms under favourable conditions."
    },
    {
      "id": "2411.14405v1",
      "title": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
      "author": "Yu Zhao",
      "coauthors": [
        "Huifeng Yin",
        "Bo Zeng",
        "Hao Wang",
        "Tianqi Shi",
        "Chenyang Lyu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Currently OpenAI o1 has sparked a surge of interest in the study of large\nreasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on\ndisciplines with standard answers, such as mathematics, physics, and coding --\nwhich are well-suited for reinforcement learning (RL) -- but also places\ngreater emphasis on open-ended resolutions. We aim to address the question:\n\"Can the o1 model effectively generalize to broader domains where clear\nstandards are absent and rewards are challenging to quantify?\" Marco-o1 is\npowered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),\nreflection mechanisms, and innovative reasoning strategies -- optimized for\ncomplex real-world problem-solving tasks."
    },
    {
      "id": "2411.14398v1",
      "title": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings",
      "author": "Aaron Zheng",
      "coauthors": [
        "Mansi Rana",
        "Andreas Stolcke"
      ],
      "abstract": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark."
    },
    {
      "id": "2411.14393v1",
      "title": "POS-tagging to highlight the skeletal structure of sentences",
      "author": "Grigorii Churakov",
      "coauthors": [],
      "abstract": "This study presents the development of a part-of-speech (POS) tagging model\nto extract the skeletal structure of sentences using transfer learning with the\nBERT architecture for token classification. The model, fine-tuned on Russian\ntext, demonstrating its effectiveness. The approach offers potential\napplications in enhancing natural language processing tasks, such as improving\nmachine translation.\n  Keywords: part of speech tagging, morphological analysis, natural language\nprocessing, BERT."
    },
    {
      "id": "2410.16162v2",
      "title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning",
      "author": "Yihong Tang",
      "coauthors": [
        "Ao Qu",
        "Zhaokai Wang",
        "Dingyi Zhuang",
        "Zhaofeng Wu",
        "Wei Ma",
        "Shenhao Wang",
        "Yunhan Zheng",
        "Zhan Zhao",
        "Jinhua Zhao"
      ],
      "abstract": "Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, most of\nthese tasks rely on the core spatial reasoning capabilities in two-dimensional\n(2D) environments, and our evaluation reveals that state-of-the-art VLMs\nfrequently generate implausible and incorrect responses to composite spatial\nreasoning problems, including simple pathfinding tasks that humans can solve\neffortlessly at a glance. To address this, we explore an effective approach to\nenhance 2D spatial reasoning within VLMs by training the model solely on basic\nspatial capabilities. We begin by disentangling the key components of 2D\nspatial reasoning: direction comprehension, distance estimation, and\nlocalization. Our central hypothesis is that mastering these basic spatial\ncapabilities can significantly enhance a model's performance on composite\nspatial tasks requiring advanced spatial understanding and combinatorial\nproblem-solving, with generalized improvements in visual-spatial tasks. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks.\nThese findings underscore the effectiveness of mastering basic spatial\ncapabilities in enhancing composite spatial problem-solving, offering insights\ninto systematic strategies for improving VLMs' spatial reasoning capabilities."
    },
    {
      "id": "2408.14512v2",
      "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings",
      "author": "Duo Wang",
      "coauthors": [
        "Yuan Zuo",
        "Fengzhi Li",
        "Junjie Wu"
      ],
      "abstract": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors."
    },
    {
      "id": "2411.14343v1",
      "title": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages",
      "author": "Bethel Melesse Tessema",
      "coauthors": [
        "Akhil Kedia",
        "Tae-Sun Chung"
      ],
      "abstract": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl."
    },
    {
      "id": "2411.14318v1",
      "title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for\n  Continual Pre-training",
      "author": "Zheheng Luo",
      "coauthors": [
        "Xin Zhang",
        "Xiao Liu",
        "Haoling Li",
        "Yeyun Gong",
        "Chen Qi",
        "Peng Cheng"
      ],
      "abstract": "It is well-known that a diverse corpus is critical for training large\nlanguage models, which are typically constructed from a mixture of various\ndomains. In general, previous efforts resort to sampling training data from\ndifferent domains with static proportions, as well as adjusting data\nproportions during training. However, few methods have addressed the\ncomplexities of domain-adaptive continual pre-training. To fill this gap, we\npropose Velocitune, a novel framework dynamically assesses learning velocity\nand adjusts data proportions accordingly, favoring slower-learning domains\nwhile shunning faster-learning ones, which is guided by a scaling law to\nindicate the desired learning goal for each domain with less associated cost.\nTo evaluate the effectiveness of Velocitune, we conduct experiments in a\nreasoning-focused dataset with CodeLlama, as well as in a corpus specialised\nfor system command generation with Llama3 and Mistral. Velocitune achieves\nperformance gains in both math and code reasoning tasks and command-line\ngeneration benchmarks. Further analysis reveals that key factors driving\nVelocitune's effectiveness include target loss prediction and data ordering."
    },
    {
      "id": "2411.13009v2",
      "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts",
      "author": "Zhuohan Gu",
      "coauthors": [
        "Jiayi Yao",
        "Kuntai Du",
        "Junchen Jiang"
      ],
      "abstract": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods."
    },
    {
      "id": "2410.16520v2",
      "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
      "author": "Naba Rizvi",
      "coauthors": [
        "Harper Strickland",
        "Daniel Gitelman",
        "Tristan Cooper",
        "Alexis Morales-Flores",
        "Michael Golden",
        "Aekta Kallepalli",
        "Akshat Alurkar",
        "Haaset Owens",
        "Saleha Ahmedi",
        "Isha Khirwadkar",
        "Imani Munyaka",
        "Nedjma Ousidhoum"
      ],
      "abstract": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
    },
    {
      "id": "2411.14279v1",
      "title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance",
      "author": "Haozhe Zhao",
      "coauthors": [
        "Shuzheng Si",
        "Liang Chen",
        "Yichi Zhang",
        "Maosong Sun",
        "Mingjia Zhang",
        "Baobao Chang"
      ],
      "abstract": "Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io)."
    },
    {
      "id": "2408.04197v2",
      "title": "Pairwise Judgment Formulation for Semantic Embedding Model in Web Search",
      "author": "Mengze Hong",
      "coauthors": [
        "Wailing Ng",
        "Zichang Guo",
        "Chen Jason Zhang"
      ],
      "abstract": "Semantic Embedding Model (SEM), a neural network-based Siamese architecture,\nis gaining momentum in information retrieval and natural language processing.\nIn order to train SEM in a supervised fashion for Web search, the search engine\nquery log is typically utilized to automatically formulate pairwise judgments\nas training data. Despite the growing application of semantic embeddings in the\nsearch engine industry, little work has been done on formulating effective\npairwise judgments for training SEM. In this paper, we make the first in-depth\ninvestigation of a wide range of strategies for generating pairwise judgments\nfor SEM. An interesting (perhaps surprising) discovery reveals that the\nconventional pairwise judgment formulation strategy wildly used in the field of\npairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.\nThrough a large-scale empirical study based on query logs and click-through\nactivities from a major commercial search engine, we demonstrate the effective\nstrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,\nClicked > Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked >\nSkipped) in LTR. We conclude with best practices for training SEM and offer\npromising insights for future research."
    },
    {
      "id": "2411.05930v2",
      "title": "BERTrend: Neural Topic Modeling for Emerging Trends Detection",
      "author": "Allaa Boutaleb",
      "coauthors": [
        "Jerome Picault",
        "Guillaume Grosjean"
      ],
      "abstract": "Detecting and tracking emerging trends and weak signals in large, evolving\ntext corpora is vital for applications such as monitoring scientific\nliterature, managing brand reputation, surveilling critical infrastructure and\nmore generally to any kind of text-based event detection. Existing solutions\noften fail to capture the nuanced context or dynamically track evolving\npatterns over time. BERTrend, a novel method, addresses these limitations using\nneural topic modeling in an online setting. It introduces a new metric to\nquantify topic popularity over time by considering both the number of documents\nand update frequency. This metric classifies topics as noise, weak, or strong\nsignals, flagging emerging, rapidly growing topics for further investigation.\nExperimentation on two large real-world datasets demonstrates BERTrend's\nability to accurately detect and track meaningful weak signals while filtering\nout noise, offering a comprehensive solution for monitoring emerging trends in\nlarge-scale, evolving text corpora. The method can also be used for\nretrospective analysis of past events. In addition, the use of Large Language\nModels together with BERTrend offers efficient means for the interpretability\nof trends of events."
    },
    {
      "id": "2411.14199v1",
      "title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented\n  LMs",
      "author": "Akari Asai",
      "coauthors": [
        "Jacqueline He",
        "Rulin Shao",
        "Weijia Shi",
        "Amanpreet Singh",
        "Joseph Chee Chang",
        "Kyle Lo",
        "Luca Soldaini",
        "Sergey Feldman",
        "Mike D'arcy",
        "David Wadden",
        "Matt Latzke",
        "Minyang Tian",
        "Pan Ji",
        "Shengyan Liu",
        "Hao Tong",
        "Bohao Wu",
        "Yanyu Xiong",
        "Luke Zettlemoyer",
        "Graham Neubig",
        "Dan Weld",
        "Doug Downey",
        "Wen-tau Yih",
        "Pang Wei Koh",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Scientific progress depends on researchers' ability to synthesize the growing\nbody of literature. Can large language models (LMs) assist scientists in this\ntask? We introduce OpenScholar, a specialized retrieval-augmented LM that\nanswers scientific queries by identifying relevant passages from 45 million\nopen-access papers and synthesizing citation-backed responses. To evaluate\nOpenScholar, we develop ScholarQABench, the first large-scale multi-domain\nbenchmark for literature search, comprising 2,967 expert-written queries and\n208 long-form answers across computer science, physics, neuroscience, and\nbiomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and\nPaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o\nhallucinates citations 78 to 90% of the time, OpenScholar achieves citation\naccuracy on par with human experts. OpenScholar's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance,\nOpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,\nexperts preferred OpenScholar-8B and OpenScholar-GPT4o responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT4o's\n32%. We open-source all of our code, models, datastore, data and a public demo."
    },
    {
      "id": "2410.18097v3",
      "title": "RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine",
      "author": "Nayoung Choi",
      "coauthors": [
        "Youngjune Lee",
        "Gyu-Hwung Cho",
        "Haeyu Jeong",
        "Jungmin Kong",
        "Saehun Kim",
        "Keunchan Park",
        "Sarah Cho",
        "Inchang Jeong",
        "Gyohee Nam",
        "Sunghoon Han",
        "Wonil Yang",
        "Jaeho Choi"
      ],
      "abstract": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries."
    },
    {
      "id": "2411.14100v1",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken\n  Term Detection",
      "author": "Anup Singh",
      "coauthors": [
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "abstract": "Spoken term detection (STD) is often hindered by reliance on frame-level\nfeatures and the computationally intensive DTW-based template matching,\nlimiting its practicality. To address these challenges, we propose a novel\napproach that encodes speech into discrete, speaker-agnostic semantic tokens.\nThis facilitates fast retrieval using text-based search algorithms and\neffectively handles out-of-vocabulary terms. Our approach focuses on generating\nconsistent token sequences across varying utterances of the same term. We also\npropose a bidirectional state space modeling within the Mamba encoder, trained\nin a self-supervised learning framework, to learn contextual frame-level\nfeatures that are further encoded into discrete tokens. Our analysis shows that\nour speech tokens exhibit greater speaker invariance than those from existing\ntokenizers, making them more suitable for STD tasks. Empirical evaluation on\nLibriSpeech and TIMIT databases indicates that our method outperforms existing\nSTD baselines while being more efficient."
    },
    {
      "id": "2402.17497v2",
      "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering",
      "author": "Yuhao Wang",
      "coauthors": [
        "Ruiyang Ren",
        "Junyi Li",
        "Wayne Xin Zhao",
        "Jing Liu",
        "Ji-Rong Wen"
      ],
      "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR."
    },
    {
      "id": "2411.13892v1",
      "title": "Topology-Aware Popularity Debiasing via Simplicial Complexes",
      "author": "Yanbiao Ji",
      "coauthors": [
        "Yue Ding",
        "Chang Liu",
        "Yuxiang Lu",
        "Xin Xin",
        "Hongtao Lu"
      ],
      "abstract": "Recommender systems (RS) play a critical role in delivering personalized\ncontent across various online platforms, leveraging collaborative filtering\n(CF) as a key technique to generate recommendations based on users' historical\ninteraction data. Recent advancements in CF have been driven by the adoption of\nGraph Neural Networks (GNNs), which model user-item interactions as bipartite\ngraphs, enabling the capture of high-order collaborative signals. Despite their\nsuccess, GNN-based methods face significant challenges due to the inherent\npopularity bias in the user-item interaction graph's topology, leading to\nskewed recommendations that favor popular items over less-known ones.\n  To address this challenge, we propose a novel topology-aware popularity\ndebiasing framework, Test-time Simplicial Propagation (TSP), which incorporates\nsimplicial complexes (SCs) to enhance the expressiveness of GNNs. Unlike\ntraditional methods that focus on pairwise relationships, our approach captures\nmulti-order relationships through SCs, providing a more comprehensive\nrepresentation of user-item interactions. By enriching the neighborhoods of\ntail items and leveraging SCs for feature smoothing, TSP enables the\npropagation of multi-order collaborative signals and effectively mitigates\nbiased propagation.\n  Our TSP module is designed as a plug-and-play solution, allowing for seamless\nintegration into pre-trained GNN-based models without the need for fine-tuning\nadditional parameters. Extensive experiments on five real-world datasets\ndemonstrate the superior performance of our method, particularly in long-tail\nrecommendation tasks. Visualization results further confirm that TSP produces\nmore uniform distributions of item representations, leading to fairer and more\naccurate recommendations."
    },
    {
      "id": "2411.13865v1",
      "title": "HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation\n  in Recommender Systems",
      "author": "Qiyao Ma",
      "coauthors": [
        "Menglin Yang",
        "Mingxuan Ju",
        "Tong Zhao",
        "Neil Shah",
        "Rex Ying"
      ],
      "abstract": "Modern recommendation systems often create information cocoons, limiting\nusers' exposure to diverse content. To enhance user experience, a crucial\nchallenge is developing systems that can balance content exploration and\nexploitation, allowing users to adjust their recommendation preferences.\nIntuitively, this balance can be achieved through a tree-structured\nrepresentation, where depth search facilitates exploitation and breadth search\nenables exploration. However, current works face two challenges to achieve this\ntarget: (1) Euclidean methods fail to fully capture hierarchical structures and\nlack flexibility in balancing exploration-exploitation, while (2) hyperbolic\napproaches, despite better hierarchical modeling, suffer from insufficient\nsemantic alignment due to their reliance on Euclidean text encoders. To address\nthese challenges, we propose HARec, a hyperbolic representation learning\nframework that jointly aligns user-item collaborative information with textual\ndescriptions in hyperbolic space. Our framework introduces two key technique\nnovelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables\nbetter hierarchical representation, and (2) a hyperbolic hierarchical tree\nstructure that facilitates user-adjustable exploration-exploitation trade-offs.\nExtensive experiments demonstrate that HARec consistently outperforms both\nEuclidean and hyperbolic baselines, achieving up to 5.49% improvement in\nutility metrics and 11.39% increase in diversity metrics."
    },
    {
      "id": "2411.11240v2",
      "title": "Controlling Diversity at Inference: Guiding Diffusion Recommender Models\n  with Targeted Category Preferences",
      "author": "Gwangseok Han",
      "coauthors": [
        "Wonbin Kweon",
        "Minsoo Kim",
        "Hwanjo Yu"
      ],
      "abstract": "Diversity control is an important task to alleviate bias amplification and\nfilter bubble problems. The desired degree of diversity may fluctuate based on\nusers' daily moods or business strategies. However, existing methods for\ncontrolling diversity often lack flexibility, as diversity is decided during\ntraining and cannot be easily modified during inference. We propose\n\\textbf{D3Rec} (\\underline{D}isentangled \\underline{D}iffusion model for\n\\underline{D}iversified \\underline{Rec}ommendation), an end-to-end method that\ncontrols the accuracy-diversity trade-off at inference. D3Rec meets our three\ndesiderata by (1) generating recommendations based on category preferences, (2)\ncontrolling category preferences during the inference phase, and (3) adapting\nto arbitrary targeted category preferences. In the forward process, D3Rec\nremoves category preferences lurking in user interactions by adding noises.\nThen, in the reverse process, D3Rec generates recommendations through denoising\nsteps while reflecting desired category preferences. Extensive experiments on\nreal-world and synthetic datasets validate the effectiveness of D3Rec in\ncontrolling diversity at inference."
    },
    {
      "id": "2406.17335v2",
      "title": "A Thorough Performance Benchmarking on Lightweight Embedding-based\n  Recommender Systems",
      "author": "Hung Vinh Tran",
      "coauthors": [
        "Tong Chen",
        "Quoc Viet Hung Nguyen",
        "Zi Huang",
        "Lizhen Cui",
        "Hongzhi Yin"
      ],
      "abstract": "Since the creation of the Web, recommender systems (RSs) have been an\nindispensable mechanism in information filtering. State-of-the-art RSs\nprimarily depend on categorical features, which ecoded by embedding vectors,\nresulting in excessively large embedding tables. To prevent over-parameterized\nembedding tables from harming scalability, both academia and industry have seen\nincreasing efforts in compressing RS embeddings. However, despite the\nprosperity of lightweight embedding-based RSs (LERSs), a wide diversity is seen\nin evaluation protocols, resulting in obstacles when relating LERS performance\nto real-world usability. Moreover, despite the common goal of lightweight\nembeddings, LERSs are evaluated with a single choice between the two main\nrecommendation tasks -- collaborative filtering and content-based\nrecommendation. This lack of discussions on cross-task transferability hinders\nthe development of unified, more scalable solutions. Motivated by these issues,\nthis study investigates various LERSs' performance, efficiency, and cross-task\ntransferability via a thorough benchmarking process. Additionally, we propose\nan efficient embedding compression method using magnitude pruning, which is an\neasy-to-deploy yet highly competitive baseline that outperforms various complex\nLERSs. Our study reveals the distinct performance of LERSs across the two\ntasks, shedding light on their effectiveness and generalizability. To support\nedge-based recommendations, we tested all LERSs on a Raspberry Pi 4, where the\nefficiency bottleneck is exposed. Finally, we conclude this paper with critical\nsummaries of LERS performance, model selection suggestions, and underexplored\nchallenges around LERSs for future research. To encourage future research, we\npublish source codes and artifacts at \\href{this\nlink}{https://github.com/chenxing1999/recsys-benchmark}."
    },
    {
      "id": "2411.14372v1",
      "title": "A Case Study on Numerical Analysis of a Path Computation Algorithm",
      "author": "Gr\u00e9goire Boussu",
      "coauthors": [
        "Nikolai Kosmatov",
        "Franck V\u00e9drine"
      ],
      "abstract": "Lack of numerical precision in control software -- in particular, related to\ntrajectory computation -- can lead to incorrect results with costly or even\ncatastrophic consequences. Various tools have been proposed to analyze the\nprecision of program computations. This paper presents a case study on\nnumerical analysis of an industrial implementation of the fast marching\nalgorithm, a popular path computation algorithm frequently used for trajectory\ncomputation. We briefly describe the selected tools, present the applied\nmethodology, highlight some attention points, summarize the results and outline\nfuture work directions."
    },
    {
      "id": "2411.14369v1",
      "title": "Model Checking and Verification of Synchronisation Properties of Cobot\n  Welding",
      "author": "Yvonne Murray",
      "coauthors": [
        "Henrik Nordlie",
        "David A. Anisi",
        "Pedro Ribeiro",
        "Ana Cavalcanti"
      ],
      "abstract": "This paper describes use of model checking to verify synchronisation\nproperties of an industrial welding system consisting of a cobot arm and an\nexternal turntable. The robots must move synchronously, but sometimes get out\nof synchronisation, giving rise to unsatisfactory weld qualities in problem\nareas, such as around corners. These mistakes are costly, since time is lost\nboth in the robotic welding and in manual repairs needed to improve the weld.\nVerification of the synchronisation properties has shown that they are\nfulfilled as long as assumptions of correctness made about parts outside the\nscope of the model hold, indicating limitations in the hardware. These results\nhave indicated the source of the problem, and motivated a re-calibration of the\nreal-life system. This has drastically improved the welding results, and is a\ndemonstration of how formal methods can be useful in an industrial setting."
    },
    {
      "id": "2411.14368v1",
      "title": "RV4Chatbot: Are Chatbots Allowed to Dream of Electric Sheep?",
      "author": "Andrea Gatti",
      "coauthors": [
        "Viviana Mascardi",
        "Angelo Ferrando"
      ],
      "abstract": "Chatbots have become integral to various application domains, including those\nwith safety-critical considerations. As a result, there is a pressing need for\nmethods that ensure chatbots consistently adhere to expected, safe behaviours.\nIn this paper, we introduce RV4Chatbot, a Runtime Verification framework\ndesigned to monitor deviations in chatbot behaviour. We formalise expected\nbehaviours as interaction protocols between the user and the chatbot. We\npresent the RV4Chatbot design and describe two implementations that instantiate\nit: RV4Rasa, for monitoring chatbots created with the Rasa framework, and\nRV4Dialogflow, for monitoring Dialogflow chatbots. Additionally, we detail\nexperiments conducted in a factory automation scenario using both RV4Rasa and\nRV4Dialogflow."
    },
    {
      "id": "2411.14367v1",
      "title": "ROSMonitoring 2.0: Extending ROS Runtime Verification to Services and\n  Ordered Topics",
      "author": "Maryam Ghaffari Saadat",
      "coauthors": [
        "Angelo Ferrando",
        "Louise A. Dennis",
        "Michael Fisher"
      ],
      "abstract": "Formal verification of robotic applications presents challenges due to their\nhybrid nature and distributed architecture. This paper introduces ROSMonitoring\n2.0, an extension of ROSMonitoring designed to facilitate the monitoring of\nboth topics and services while considering the order in which messages are\npublished and received. The framework has been enhanced to support these novel\nfeatures for ROS1 -- and partially ROS2 environments -- offering improved\nreal-time support, security, scalability, and interoperability. We discuss the\nmodifications made to accommodate these advancements and present results\nobtained from a case study involving the runtime monitoring of specific\ncomponents of a fire-fighting Uncrewed Aerial Vehicle (UAV)."
    },
    {
      "id": "2411.14303v1",
      "title": "Automated Generation of Code Debugging Exercises",
      "author": "Victor-Alexandru P\u0103durean",
      "coauthors": [
        "Paul Denny",
        "Adish Singla"
      ],
      "abstract": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging."
    },
    {
      "id": "2403.11585v3",
      "title": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines",
      "author": "Ekaterina Trofimova",
      "coauthors": [
        "Emil Sataev",
        "Andrey E. Ustyuzhanin"
      ],
      "abstract": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields."
    },
    {
      "id": "2411.14174v1",
      "title": "Translating C To Rust: Lessons from a User Study",
      "author": "Ruishi Li",
      "coauthors": [
        "Bo Wang",
        "Tianyu Li",
        "Prateek Saxena",
        "Ashish Kundu"
      ],
      "abstract": "Rust aims to offer full memory safety for programs, a guarantee that untamed\nC programs do not enjoy. How difficult is it to translate existing C code to\nRust? To get a complementary view from that of automatic C to Rust translators,\nwe report on a user study asking humans to translate real-world C programs to\nRust. Our participants are able to produce safe Rust translations, whereas\nstate-of-the-art automatic tools are not able to do so. Our analysis highlights\nthat the high-level strategy taken by users departs significantly from those of\nautomatic tools we study. We also find that users often choose zero-cost\n(static) abstractions for temporal safety, which addresses a predominant\ncomponent of runtime costs in other full memory safety defenses. User-provided\ntranslations showcase a rich landscape of specialized strategies to translate\nthe same C program in different ways to safe Rust, which future automatic\ntranslators can consider."
    },
    {
      "id": "2309.16584v4",
      "title": "Collaborative Distributed Machine Learning",
      "author": "David Jin",
      "coauthors": [
        "Niclas Kannengie\u00dfer",
        "Sascha Rank",
        "Ali Sunyaev"
      ],
      "abstract": "Various collaborative distributed machine learning (CDML) systems, including\nfederated learning systems and swarm learning systems, with diferent key traits\nwere developed to leverage resources for the development and use of machine\nlearning(ML) models in a conidentiality-preserving way. To meet use case\nrequirements, suitable CDML systems need to be selected. However, comparison\nbetween CDML systems to assess their suitability for use cases is often\ndiicult. To support comparison of CDML systems and introduce scientiic and\npractical audiences to the principal functioning and key traits of CDML\nsystems, this work presents a CDML system conceptualization and CDML\narchetypes."
    },
    {
      "id": "2411.14277v1",
      "title": "Neuro-Symbolic Query Optimization in Knowledge Graphs",
      "author": "Maribel Acosta",
      "coauthors": [
        "Chang Qin",
        "Tim Schwabe"
      ],
      "abstract": "This chapter delves into the emerging field of neuro-symbolic query\noptimization for knowledge graphs (KGs), presenting a comprehensive exploration\nof how neural and symbolic techniques can be integrated to enhance query\nprocessing. Traditional query optimizers in knowledge graphs rely heavily on\nsymbolic methods, utilizing dataset summaries, statistics, and cost models to\nselect efficient execution plans. However, these approaches often suffer from\nmisestimations and inaccuracies, particularly when dealing with complex queries\nor large-scale datasets. Recent advancements have introduced neural models,\nwhich capture non-linear aspects of query optimization, offering promising\nalternatives to purely symbolic methods. In this chapter, we introduce\nneuro-symbolic query optimizers, a novel approach that combines the strengths\nof symbolic reasoning with the adaptability of neural computation. We discuss\nthe architecture of these hybrid systems, highlighting the interplay between\nneural and symbolic components to improve the optimizer's ability to navigate\nthe search space and produce efficient execution plans. Additionally, the\nchapter reviews existing neural components tailored for optimizing queries over\nknowledge graphs and examines the limitations and challenges in deploying\nneuro-symbolic query optimizers in real-world environments."
    },
    {
      "id": "2405.07460v4",
      "title": "HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology\n  Datasets with Foundational Embedding Models",
      "author": "Aakash Tripathi",
      "coauthors": [
        "Asim Waqas",
        "Matthew B. Schabath",
        "Yasin Yilmaz",
        "Ghulam Rasool"
      ],
      "abstract": "Developing accurate machine learning models for oncology requires\nlarge-scale, high-quality multimodal datasets. However, creating such datasets\nremains challenging due to the complexity and heterogeneity of medical data. To\naddress this challenge, we introduce HoneyBee, a scalable modular framework for\nbuilding multimodal oncology datasets that leverages foundation models to\ngenerate representative embeddings. HoneyBee integrates various data\nmodalities, including clinical diagnostic and pathology imaging data, medical\nnotes, reports, records, and molecular data. It employs data preprocessing\ntechniques and foundation models to generate embeddings that capture the\nessential features and relationships within the raw medical data. The generated\nembeddings are stored in a structured format using Hugging Face datasets and\nPyTorch dataloaders for accessibility. Vector databases enable efficient\nquerying and retrieval for machine learning applications. We demonstrate the\neffectiveness of HoneyBee through experiments assessing the quality and\nrepresentativeness of these embeddings. The framework is designed to be\nextensible to other medical domains and aims to accelerate oncology research by\nproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoing\nopen-source effort, and the code, datasets, and models are available at the\nproject repository."
    },
    {
      "id": "2303.05327v2",
      "title": "Direct Access for Answers to Conjunctive Queries with Aggregation",
      "author": "Idan Eldar",
      "coauthors": [
        "Nofar Carmeli",
        "Benny Kimelfeld"
      ],
      "abstract": "We study the fine-grained complexity of conjunctive queries with grouping and\naggregation. For common aggregate functions (e.g., min, max, count, sum), such\na query can be phrased as an ordinary conjunctive query over a database\nannotated with a suitable commutative semiring. We investigate the ability to\nevaluate such queries by constructing in loglinear time a data structure that\nprovides logarithmic-time direct access to the answers ordered by a given\nlexicographic order. This task is nontrivial since the number of answers might\nbe larger than loglinear in the size of the input, so the data structure needs\nto provide a compact representation of the space of answers. In the absence of\naggregation and annotation, past research established a sufficient tractability\ncondition on queries and orders. For queries without self-joins, this condition\nis not just sufficient, but also necessary (under conventional lower-bound\nassumptions in fine-grained complexity).\n  We show that all past results continue to hold for annotated databases,\nassuming that the annotation itself does not participate in the lexicographic\norder. Yet, past algorithms do not apply to the count-distinct aggregation,\nwhich has no efficient representation as a commutative semiring; for this\naggregation, we establish the corresponding tractability condition. We then\nshow how the complexity of the problem changes when we include the aggregate\nand annotation value in the order. We also study the impact of having all\nrelations but one annotated by the multiplicative identity (one), as happens\nwhen we translate aggregate queries into semiring annotations, and having a\nsemiring with an idempotent addition, such as the case of min, max, and\ncount-distinct over a logarithmic-size domain."
    },
    {
      "id": "2312.11122v3",
      "title": "Evaluation of Dataframe Libraries for Data Preparation on a Single\n  Machine",
      "author": "Angelo Mozzillo",
      "coauthors": [
        "Luca Zecchini",
        "Luca Gagliardelli",
        "Adeel Aslam",
        "Sonia Bergamaschi",
        "Giovanni Simonini"
      ],
      "abstract": "Data preparation is a trial-and-error process that typically involves\ncountless iterations over the data to define the best pipeline of operators for\na given task. With tabular data, practitioners often perform that burdensome\nactivity on local machines by writing ad hoc scripts with libraries based on\nthe Pandas dataframe API and testing them on samples of the entire dataset-the\nfaster the library, the less idle time its users have.\n  In this paper, we evaluate the most popular Python dataframe libraries in\ngeneral data preparation use cases to assess how they perform on a single\nmachine. To do so, we employ 4 real-world datasets with heterogeneous features,\ncovering a variety of scenarios, and the TPC-H benchmark. The insights gained\nwith this experimentation are useful to data scientists who need to choose\nwhich of the dataframe libraries best suits their data preparation task at\nhand.\n  In a nutshell, we found that: for small datasets, Pandas consistently proves\nto be the best choice with the richest API; when data fits in RAM and there is\nno need for complete compatibility with Pandas API, Polars is the go-to choice\nthanks to its in-memory execution and query optimizations; when a GPU is\navailable, CuDF often yields the best performance, while for very large\ndatasets that cannot fit in the GPU memory and RAM, PySpark (thanks to a\nmultithread execution and a query optimizer) proves to be the best option."
    },
    {
      "id": "2411.13704v1",
      "title": "Towards Query Optimizer as a Service (QOaaS) in a Unified LakeHouse\n  Ecosystem: Can One QO Rule Them All?",
      "author": "Rana Alotaibi",
      "coauthors": [
        "Yuanyuan Tian",
        "Stefan Grafberger",
        "Jes\u00fas Camacho-Rodr\u00edguez",
        "Nicolas Bruno",
        "Brian Kroth",
        "Sergiy Matusevych",
        "Ashvin Agrawal",
        "Mahesh Behera",
        "Ashit Gosalia",
        "Cesar Galindo-Legaria",
        "Milind Joshi",
        "Milan Potocnik",
        "Beysim Sezgin",
        "Xiaoyu Li",
        "Carlo Curino"
      ],
      "abstract": "Customer demand, regulatory pressure, and engineering efficiency are the\ndriving forces behind the industry-wide trend of moving from siloed engines and\nservices that are optimized in isolation to highly integrated solutions. This\nis confirmed by the wide adoption of open formats, shared component libraries,\nand the meteoric success of integrated data lake experiences such as Microsoft\nFabric.\n  In this paper, we study the implications of this trend to Query Optimizer\n(QO) and discuss our experience of building Calcite and extending Cascades into\nQO components of Microsoft SQL Server, Fabric Data Warehouse (DW), and SCOPE.\nWe weigh the pros and cons of a drastic change in direction: moving from\nbespoke QOs or library-sharing (\\`a la Calcite) to rewriting the QO stack and\nfully embracing Query Optimizer as a Service (QOaaS). We report on some early\nsuccesses and stumbles as we explore these ideas with prototypes compatible\nwith Fabric DW and Spark. The benefits include centralized workload-level\noptimizations, multi-engine federation, and accelerated feature creation, but\nthe challenges are equally daunting. We plan to engage CIDR audience in a\ndebate on this exciting topic."
    },
    {
      "id": "2407.15462v3",
      "title": "Retrieval with Learned Similarities",
      "author": "Bailu Ding",
      "coauthors": [
        "Jiaqi Zhai"
      ],
      "abstract": "Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing (NLP) by efficiently finding relevant items from a\nlarge corpus given a query. Dot products have been widely used as the\nsimilarity function in such tasks, enabled by Maximum Inner Product Search\n(MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval\nalgorithms have migrated to learned similarities. These advanced approaches\nencompass multiple query embeddings, complex neural networks, direct item ID\ndecoding via beam search, and hybrid solutions. Unfortunately, we lack\nefficient solutions for retrieval in these state-of-the-art setups. Our work\naddresses this gap by investigating efficient retrieval techniques with\nexpressive learned similarity functions. We establish Mixture-of-Logits (MoL)\nas a universal approximator of similarity functions, demonstrate that MoL's\nexpressiveness can be realized empirically to achieve superior performance on\ndiverse retrieval scenarios, and propose techniques to retrieve the approximate\ntop-k results using MoL with tight error bounds. Through extensive\nexperimentation, we show that MoL, enhanced by our proposed mutual\ninformation-based load balancing loss, sets new state-of-the-art results across\nheterogeneous scenarios, including sequential retrieval models in\nrecommendation systems and finetuning language models for question answering;\nand our approximate top-$k$ algorithms outperform baselines by up to 66x in\nlatency while achieving >.99 recall rate compared to exact algorithms."
    },
    {
      "id": "2101.03712v4",
      "title": "Enumeration Algorithms for Conjunctive Queries with Projection",
      "author": "Shaleen Deep",
      "coauthors": [
        "Xiao Hu",
        "Paraschos Koutris"
      ],
      "abstract": "We investigate the enumeration of query results for an important subset of\nCQs with projections, namely star and path queries. The task is to design data\nstructures and algorithms that allow for efficient enumeration with delay\nguarantees after a preprocessing phase. Our main contribution is a series of\nresults based on the idea of interleaving precomputed output with further join\nprocessing to maintain delay guarantees, which maybe of independent interest.\nIn particular, for star queries, we design combinatorial algorithms that\nprovide instance-specific delay guarantees in linear preprocessing time. These\nalgorithms improve upon the currently best known results. Further, we show how\nexisting results can be improved upon by using fast matrix multiplication. We\nalso present new results involving tradeoff between preprocessing time and\ndelay guarantees for enumeration of path queries that contain projections.\nBoolean matrix multiplication is an important query that can be expressed as a\nCQ with projection where the join attribute is projected away. Our results can\ntherefore also be interpreted as sparse, output-sensitive matrix multiplication\nwith delay guarantees."
    },
    {
      "id": "2411.14424v1",
      "title": "Learning Fair Robustness via Domain Mixup",
      "author": "Meiyu Zhong",
      "coauthors": [
        "Ravi Tandon"
      ],
      "abstract": "Adversarial training is one of the predominant techniques for training\nclassifiers that are robust to adversarial attacks. Recent work, however has\nfound that adversarial training, which makes the overall classifier robust, it\ndoes not necessarily provide equal amount of robustness for all classes. In\nthis paper, we propose the use of mixup for the problem of learning fair robust\nclassifiers, which can provide similar robustness across all classes.\nSpecifically, the idea is to mix inputs from the same classes and perform\nadversarial training on mixed up inputs. We present a theoretical analysis of\nthis idea for the case of linear classifiers and show that mixup combined with\nadversarial training can provably reduce the class-wise robustness disparity.\nThis method not only contributes to reducing the disparity in class-wise\nadversarial risk, but also the class-wise natural risk. Complementing our\ntheoretical analysis, we also provide experimental results on both synthetic\ndata and the real world dataset (CIFAR-10), which shows improvement in class\nwise disparities for both natural and adversarial risks."
    },
    {
      "id": "2411.14412v1",
      "title": "Adversarial Poisoning Attack on Quantum Machine Learning Models",
      "author": "Satwik Kundu",
      "coauthors": [
        "Swaroop Ghosh"
      ],
      "abstract": "With the growing interest in Quantum Machine Learning (QML) and the\nincreasing availability of quantum computers through cloud providers,\naddressing the potential security risks associated with QML has become an\nurgent priority. One key concern in the QML domain is the threat of data\npoisoning attacks in the current quantum cloud setting. Adversarial access to\ntraining data could severely compromise the integrity and availability of QML\nmodels. Classical data poisoning techniques require significant knowledge and\ntraining to generate poisoned data, and lack noise resilience, making them\nineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era.\nIn this work, we first propose a simple yet effective technique to measure\nintra-class encoder state similarity (ESS) by analyzing the outputs of encoding\ncircuits. Leveraging this approach, we introduce a quantum indiscriminate data\npoisoning attack, QUID. Through extensive experiments conducted in both\nnoiseless and noisy environments (e.g., IBM\\_Brisbane's noise), across various\narchitectures and datasets, QUID achieves up to $92\\%$ accuracy degradation in\nmodel performance compared to baseline models and up to $75\\%$ accuracy\ndegradation compared to random label-flipping. We also tested QUID against\nstate-of-the-art classical defenses, with accuracy degradation still exceeding\n$50\\%$, demonstrating its effectiveness. This work represents the first attempt\nto reevaluate data poisoning attacks in the context of QML."
    },
    {
      "id": "2411.14394v1",
      "title": "Securing Legacy Communication Networks via Authenticated Cyclic\n  Redundancy Integrity Check",
      "author": "Alessandro Lotto",
      "coauthors": [
        "Alessandro Brighente",
        "Mauro Conti"
      ],
      "abstract": "Integrating modern communication technologies into legacy systems, such as\nIndustrial Control Systems and in-vehicle networks, invalidates the assumptions\nof isolated and trusted operating environments. Security incidents like the\n2015 Ukraine power grid attack and the 2021 compromise of a U.S. water\ntreatment facility demonstrate how increased interconnectivity, paired with\ninsufficient security measures, expose these critical systems to cyber threats,\nposing risks to national and public safety. These attacks were favored by the\nlack of proper message authentication, highlighting its importance as a primary\ncountermeasure to enhance system security. Solutions proposed in the literature\nremain largely unadopted in practice due to challenges such as preserving\nbackward compatibility, additional hardware requirements, and limited\ncomputational resources on legacy devices. Moreover, many solutions are\nprotocol-specific, necessitating complex and costly multiple implementations in\nheterogeneous systems.\n  In this paper, we propose Authenticated Cyclic Redundancy Integrity Check\n(ACRIC), a novel security mechanism that overcomes these limitations by\nleveraging a cryptographic computation of the existing Cyclyic Redundancy Check\n(CRC) field to ensure message integrity protection and authentication. ACRIC\npreserves backward compatibility without requiring additional hardware and is\nprotocol agnostic. This makes it applicable across various systems, suitable\nfor diverse legacy network protocols including point-to-point and broadcast\ncommunications. Experimental results, supported by formal verification and\nreal-world testing, demonstrate that ACRIC offers robust security with minimal\ntransmission overhead (<< 1 ms). This proves ACRIC's practicality,\ncost-effectiveness, and suitability for real-world adoption."
    },
    {
      "id": "2405.00663v3",
      "title": "Quantum cryptographic protocols with dual messaging system via 2D\n  alternate quantum walk of a genuine single-photon entangled state",
      "author": "Dinesh Kumar Panda",
      "coauthors": [
        "Colin Benjamin"
      ],
      "abstract": "A single-photon entangled state (or single-particle entangled state (SPES) in\ngeneral) can offer a more secure way of encoding and processing quantum\ninformation than their multi-photon (or multi-particle) counterparts. The SPES\ngenerated via a 2D alternate quantum-walk setup from initially separable states\ncan be either 3-way or 2-way entangled. This letter shows that the generated\ngenuine three-way and nonlocal two-way SPES can be used as cryptographic keys\nto securely encode two distinct messages simultaneously. We detail the message\nencryption-decryption steps and show the resilience of the 3-way and 2-way\nSPES-based cryptographic protocols against eavesdropper attacks like\nintercept-and-resend and man-in-the-middle. We also detail the experimental\nrealization of these protocols using a single photon, with the three degrees of\nfreedom being OAM, path, and polarization. We have proved that the protocols\nhave unconditional security for quantum communication tasks. The ability to\nsimultaneously encode two distinct messages using the generated SPES showcases\nthe versatility and efficiency of the proposed cryptographic protocol. This\ncapability could significantly improve the throughput of quantum communication\nsystems."
    },
    {
      "id": "2308.06405v3",
      "title": "White-box Membership Inference Attacks against Diffusion Models",
      "author": "Yan Pang",
      "coauthors": [
        "Tianhao Wang",
        "Xuhui Kang",
        "Mengdi Huai",
        "Yang Zhang"
      ],
      "abstract": "Diffusion models have begun to overshadow GANs and other generative models in\nindustrial applications due to their superior image generation performance. The\ncomplex architecture of these models furnishes an extensive array of attack\nfeatures. In light of this, we aim to design membership inference attacks\n(MIAs) catered to diffusion models. We first conduct an exhaustive analysis of\nexisting MIAs on diffusion models, taking into account factors such as\nblack-box/white-box models and the selection of attack features. We found that\nwhite-box attacks are highly applicable in real-world scenarios, and the most\neffective attacks presently are white-box. Departing from earlier research,\nwhich employs model loss as the attack feature for white-box MIAs, we employ\nmodel gradients in our attack, leveraging the fact that these gradients provide\na more profound understanding of model responses to various samples. We subject\nthese models to rigorous testing across a range of parameters, including\ntraining steps, sampling frequency, diffusion steps, and data variance. Across\nall experimental settings, our method consistently demonstrated near-flawless\nattack performance, with attack success rate approaching 100% and attack AUCROC\nnear 1.0. We also evaluate our attack against common defense mechanisms, and\nobserve our attacks continue to exhibit commendable performance."
    },
    {
      "id": "2411.14278v1",
      "title": "Adaptive Anomaly Detection for Identifying Attacks in Cyber-Physical\n  Systems: A Systematic Literature Review",
      "author": "Pablo Moriano",
      "coauthors": [
        "Steven C. Hespeler",
        "Mingyan Li",
        "Maria Mahbub"
      ],
      "abstract": "Modern cyberattacks in cyber-physical systems (CPS) rapidly evolve and cannot\nbe deterred effectively with most current methods which focused on\ncharacterizing past threats. Adaptive anomaly detection (AAD) is among the most\npromising techniques to detect evolving cyberattacks focused on fast data\nprocessing and model adaptation. AAD has been researched in the literature\nextensively; however, to the best of our knowledge, our work is the first\nsystematic literature review (SLR) on the current research within this field.\nWe present a comprehensive SLR, gathering 397 relevant papers and\nsystematically analyzing 65 of them (47 research and 18 survey papers) on AAD\nin CPS studies from 2013 to 2023 (November). We introduce a novel taxonomy\nconsidering attack types, CPS application, learning paradigm, data management,\nand algorithms. Our analysis indicates, among other findings, that reviewed\nworks focused on a single aspect of adaptation (either data processing or model\nadaptation) but rarely in both at the same time. We aim to help researchers to\nadvance the state of the art and help practitioners to become familiar with\nrecent progress in this field. We identify the limitations of the state of the\nart and provide recommendations for future research directions."
    },
    {
      "id": "2404.11977v4",
      "title": "Mens Sana In Corpore Sano: Sound Firmware Corpora for Vulnerability\n  Research",
      "author": "Ren\u00e9 Helmke",
      "coauthors": [
        "Elmar Padilla",
        "Nils Aschenbruck"
      ],
      "abstract": "Firmware corpora for vulnerability research should be scientifically sound.\nYet, several practical challenges complicate the creation of sound corpora:\nSample acquisition, e.g., is hard and one must overcome the barrier of\nproprietary or encrypted data. As image contents are unknown prior analysis, it\nis hard to select high-quality samples that can satisfy scientific demands.\nIdeally, we help each other out by sharing data. But here, sharing is\nproblematic due to copyright laws. Instead, papers must carefully document each\nstep of corpus creation: If a step is unclear, replicability is jeopardized.\nThis has cascading effects on result verifiability, representativeness, and,\nthus, soundness.\n  Despite all challenges, how can we maintain the soundness of firmware\ncorpora? This paper thoroughly analyzes the problem space and investigates its\nimpact on research: We distill practical binary analysis challenges that\nsignificantly influence corpus creation. We use these insights to derive\nguidelines that help researchers to nurture corpus replicability and\nrepresentativeness. We apply them to 44 top tier papers and systematically\nanalyze scientific corpus creation practices. Our comprehensive analysis\nconfirms that there is currently no common ground in related work. It shows the\nadded value of our guidelines, as they discover methodical issues in corpus\ncreation and unveil miniscule step stones in documentation. These blur visions\non representativeness, hinder replicability, and, thus, negatively impact the\nsoundness of otherwise excellent work.\n  Finally, we show the feasibility of our guidelines and build a new,\nreplicable corpus for large-scale analyses on Linux firmware: LFwC. We share\nrich meta data for good (and proven) replicability. We verify unpacking,\ndeduplicate, identify contents, provide ground truth, and show LFwC's utility\nfor research."
    },
    {
      "id": "2411.14245v1",
      "title": "Pulsar Consensus",
      "author": "Samer Afach",
      "coauthors": [
        "Benjamin Marsh",
        "Enrico Rubboli"
      ],
      "abstract": "In this paper, we informally introduce the Pulsar proof of stake consensus\npaper and discuss the relevant design decisions and considerations. The Pulsar\nprotocol we propose is designed to facilitate the creation of a proof of stake\nsidechain for a proof of work blockchain. We present an overview of a novel\ncomposable density-based chain selection rule for proof of stake systems which\ncan be seen as a superset of some standard existing longest chain rules for\nproof of stake protocols. We discuss the Pulsar protocol in comparison to\nexisting proof of stake protocols and define its benefits over existing designs\nwhile defining the limitations of the work. Pulsar is currently implemented in\nthe Mintlayer proof of stake Bitcoin sidechain."
    },
    {
      "id": "2411.14243v1",
      "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
      "author": "Jialin Lu",
      "coauthors": [
        "Junjie Shan",
        "Ziqi Zhao",
        "Ka-Ho Chow"
      ],
      "abstract": "As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a significant threat by implanting hidden backdoor in a victim\nmodel, which adversaries can later exploit to trigger malicious behaviors\nduring inference. However, current backdoor techniques are limited to static\nscenarios where attackers must define a malicious objective before training,\nlocking the attack into a predetermined action without inference-time\nadaptability. Given the expressive output space in object detection, including\nobject existence detection, bounding box estimation, and object classification,\nthe feasibility of implanting a backdoor that provides inference-time control\nwith a high degree of freedom remains unexplored. This paper introduces\nAnywhereDoor, a flexible backdoor attack tailored for object detection. Once\nimplanted, AnywhereDoor enables adversaries to specify different attack types\n(object vanishing, fabrication, or misclassification) and configurations\n(untargeted or targeted with specific classes) to dynamically control detection\nbehavior. This flexibility is achieved through three key innovations: (i)\nobjective disentanglement to support a broader range of attack combinations\nwell beyond what existing methods allow; (ii) trigger mosaicking to ensure\nbackdoor activations are robust, even against those object detectors that\nextract localized regions from the input image for recognition; and (iii)\nstrategic batching to address object-level data imbalances that otherwise\nhinders a balanced manipulation. Extensive experiments demonstrate that\nAnywhereDoor provides attackers with a high degree of control, achieving an\nattack success rate improvement of nearly 80% compared to adaptations of\nexisting methods for such flexible control."
    },
    {
      "id": "2411.14174v1",
      "title": "Translating C To Rust: Lessons from a User Study",
      "author": "Ruishi Li",
      "coauthors": [
        "Bo Wang",
        "Tianyu Li",
        "Prateek Saxena",
        "Ashish Kundu"
      ],
      "abstract": "Rust aims to offer full memory safety for programs, a guarantee that untamed\nC programs do not enjoy. How difficult is it to translate existing C code to\nRust? To get a complementary view from that of automatic C to Rust translators,\nwe report on a user study asking humans to translate real-world C programs to\nRust. Our participants are able to produce safe Rust translations, whereas\nstate-of-the-art automatic tools are not able to do so. Our analysis highlights\nthat the high-level strategy taken by users departs significantly from those of\nautomatic tools we study. We also find that users often choose zero-cost\n(static) abstractions for temporal safety, which addresses a predominant\ncomponent of runtime costs in other full memory safety defenses. User-provided\ntranslations showcase a rich landscape of specialized strategies to translate\nthe same C program in different ways to safe Rust, which future automatic\ntranslators can consider."
    },
    {
      "id": "2406.08222v2",
      "title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion",
      "author": "Sha Luo",
      "coauthors": [
        "Sang Jung Kim",
        "Zening Duan",
        "Kaiping Chen"
      ],
      "abstract": "In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them."
    },
    {
      "id": "2411.14368v1",
      "title": "RV4Chatbot: Are Chatbots Allowed to Dream of Electric Sheep?",
      "author": "Andrea Gatti",
      "coauthors": [
        "Viviana Mascardi",
        "Angelo Ferrando"
      ],
      "abstract": "Chatbots have become integral to various application domains, including those\nwith safety-critical considerations. As a result, there is a pressing need for\nmethods that ensure chatbots consistently adhere to expected, safe behaviours.\nIn this paper, we introduce RV4Chatbot, a Runtime Verification framework\ndesigned to monitor deviations in chatbot behaviour. We formalise expected\nbehaviours as interaction protocols between the user and the chatbot. We\npresent the RV4Chatbot design and describe two implementations that instantiate\nit: RV4Rasa, for monitoring chatbots created with the Rasa framework, and\nRV4Dialogflow, for monitoring Dialogflow chatbots. Additionally, we detail\nexperiments conducted in a factory automation scenario using both RV4Rasa and\nRV4Dialogflow."
    },
    {
      "id": "2411.14298v1",
      "title": "Decoding the Meaning of Success on Digital Labor Platforms:\n  Worker-Centered Perspectives",
      "author": "Pyeonghwa Kim",
      "coauthors": [
        "Charis Asante-Agyei",
        "Isabel Munoz",
        "Michael Dunn",
        "Steve Sawyer"
      ],
      "abstract": "What does work and career success mean for those who secure their work using\ndigital labor platforms? Traditional research on success predominantly relies\non organizationally-centric benchmarks, such as promotions and income. These\nmeasures provide limited insights into the evolving nature of work and careers\nshaped at the intersection of digital labor platform technologies and workers'\nevolving perspectives. Drawing on data from a longitudinal study of 108 digital\nlabor platform workers on Upwork, we (1) identify seven dimensions of success\nindicators that reflect workers' definitions of success in platform-mediated\nwork and careers, (2) delineate three dimensions of digital labor platforms\nmediating workers' experiences of success and (3) examine the shifting\nperspectives of these workers relative to success. Based on these findings, we\ndiscuss the implications of platform-mediated success in workers' labor\nexperiences, marked by platformic management, standardization, precarity and\nongoing evolution. Our discussion intertwines CSCW scholarship with career\nstudies, advancing a more nuanced understanding of the evolving perspectives on\nsuccess in platform-mediated work and careers."
    },
    {
      "id": "2411.14233v1",
      "title": "A qualitative analysis of remote patient monitoring: how a paradox\n  mindset can support balancing emotional tensions in the design of healthcare\n  technologies",
      "author": "Zoe Jonassen",
      "coauthors": [
        "Katharine Lawrence",
        "Batia Mishan Wiesenfeld",
        "Stefan Feuerriegel",
        "Devin Mann"
      ],
      "abstract": "Remote patient monitoring (RPM) is the use of digital technologies to improve\npatient care at a distance. However, current RPM solutions are often biased\ntoward tech-savvy patients. To foster health equity, researchers have studied\nhow to address the socio-economic and cognitive needs of diverse patient\ngroups, but their emotional needs have remained largely neglected. We perform\nthe first qualitative study to explore the emotional needs of diverse patients\naround RPM. Specifically, we conduct a thematic analysis of 18 interviews and 4\nfocus groups at a large US healthcare organization. We identify emotional needs\nthat lead to four emotional tensions within and across stakeholder groups when\napplying an equity focus to the design and implementation of RPM technologies.\nThe four emotional tensions are making diverse patients feel: (i) heard vs.\nexploited; (ii) seen vs. deprioritized for efficiency; (iii) empowered vs.\nanxious; and (iv) cared for vs. detached from care. To manage these emotional\ntensions across stakeholders, we develop design recommendations informed by a\nparadox mindset (i.e., \"both-and\" rather than \"and-or\" strategies)."
    },
    {
      "id": "2411.14131v1",
      "title": "sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset,\n  Toolbox, and Benchmark Results",
      "author": "Hongxin Li",
      "coauthors": [
        "Jingsheng Tang",
        "Xuechao Xu",
        "Wei Dai",
        "Yaru Liu",
        "Junhao Xiao",
        "Huimin Lu",
        "Zongtan Zhou"
      ],
      "abstract": "In sensitive scenarios, such as meetings, negotiations, and team sports,\nmessages must be conveyed without detection by non-collaborators. Previous\nmethods, such as encrypting messages, eye contact, and micro-gestures, had\nproblems with either inaccurate information transmission or leakage of\ninteraction intentions. To this end, a novel gesture-free hand intention\nrecognition scheme was proposed, that adopted surface electromyography(sEMG)\nand isometric contraction theory to recognize different hand intentions without\nany gesture. Specifically, this work includes four parts: (1) the experimental\nsystem, consisting of the upper computer software, self-conducted myoelectric\nwatch, and sports platform, is built to get sEMG signals and simulate multiple\nusage scenarios; (2) the paradigm is designed to standard prompt and collect\nthe gesture-free sEMG datasets. Eight-channel signals of ten subjects were\nrecorded twice per subject at about 5-10 days intervals; (3) the toolbox\nintegrates preprocessing methods (data segmentation, filter, normalization,\netc.), commonly used sEMG signal decoding methods, and various plotting\nfunctions, to facilitate the research of the dataset; (4) the benchmark results\nof widely used methods are provided. The results involve single-day, cross-day,\nand cross-subject experiments of 6-class and 12-class gesture-free hand\nintention when subjects with different sports motions. To help future research,\nall data, hardware, software, and methods are open-sourced on the following\nwebsite: click here."
    },
    {
      "id": "2411.14009v1",
      "title": "GPT versus Humans: Uncovering Ethical Concerns in Conversational\n  Generative AI-empowered Multi-Robot Systems",
      "author": "Rebekah Rousi",
      "coauthors": [
        "Niko Makitalo",
        "Hooman Samani",
        "Kai-Kristian Kemell",
        "Jose Siqueira de Cerqueira",
        "Ville Vakkuri",
        "Tommi Mikkonen",
        "Pekka Abrahamsson"
      ],
      "abstract": "The emergence of generative artificial intelligence (GAI) and large language\nmodels (LLMs) such ChatGPT has enabled the realization of long-harbored desires\nin software and robotic development. The technology however, has brought with\nit novel ethical challenges. These challenges are compounded by the application\nof LLMs in other machine learning systems, such as multi-robot systems. The\nobjectives of the study were to examine novel ethical issues arising from the\napplication of LLMs in multi-robot systems. Unfolding ethical issues in GPT\nagent behavior (deliberation of ethical concerns) was observed, and GPT output\nwas compared with human experts. The article also advances a model for ethical\ndevelopment of multi-robot systems. A qualitative workshop-based method was\nemployed in three workshops for the collection of ethical concerns: two human\nexpert workshops (N=16 participants) and one GPT-agent-based workshop (N=7\nagents; two teams of 6 agents plus one judge). Thematic analysis was used to\nanalyze the qualitative data. The results reveal differences between the\nhuman-produced and GPT-based ethical concerns. Human experts placed greater\nemphasis on new themes related to deviance, data privacy, bias and unethical\ncorporate conduct. GPT agents emphasized concerns present in existing AI ethics\nguidelines. The study contributes to a growing body of knowledge in\ncontext-specific AI ethics and GPT application. It demonstrates the gap between\nhuman expert thinking and LLM output, while emphasizing new ethical concerns\nemerging in novel technology."
    },
    {
      "id": "2409.16098v2",
      "title": "The Digital Transformation in Health: How AI Can Improve the Performance\n  of Health Systems",
      "author": "\u00c1frica Peri\u00e1\u00f1ez",
      "coauthors": [
        "Ana Fern\u00e1ndez del R\u00edo",
        "Ivan Nazarov",
        "Enric Jan\u00e9",
        "Moiz Hassan",
        "Aditya Rastogi",
        "Dexian Tang"
      ],
      "abstract": "Mobile health has the potential to revolutionize health care delivery and\npatient engagement. In this work, we discuss how integrating Artificial\nIntelligence into digital health applications-focused on supply chain, patient\nmanagement, and capacity building, among other use cases-can improve the health\nsystem and public health performance. We present an Artificial Intelligence and\nReinforcement Learning platform that allows the delivery of adaptive\ninterventions whose impact can be optimized through experimentation and\nreal-time monitoring. The system can integrate multiple data sources and\ndigital health applications. The flexibility of this platform to connect to\nvarious mobile health applications and digital devices and send personalized\nrecommendations based on past data and predictions can significantly improve\nthe impact of digital tools on health system outcomes. The potential for\nresource-poor settings, where the impact of this approach on health outcomes\ncould be more decisive, is discussed specifically. This framework is, however,\nsimilarly applicable to improving efficiency in health systems where scarcity\nis not an issue."
    },
    {
      "id": "2411.13946v1",
      "title": "A Systematic Literature Review on Technology Acceptance Research on\n  Augmented Reality in the Field of Training and Education",
      "author": "Stefan Graser",
      "coauthors": [
        "Stephan B\u00f6hm"
      ],
      "abstract": "Augmented Reality (AR) is an emerging technology that ranks among the top\ninnovations in interactive media. With the emergence of new technologies, the\nquestion about the factors influencing user acceptance arises. Many research\nmodels on the user acceptance of technologies were developed and extended to\nanswer this question in the last decades. This research paper provides an\noverview of the current state in the scientific literature on user acceptance\nfactors of AR in training and education. We conducted a systematic literature\nreview, identifying 45 scientific papers on technology acceptance of augmented\nreality. Twenty-two papers refer more specifically to the field of training and\neducation. Overall, 33 different technology acceptance models and 34 acceptance\nvariables were identified. Based on the results, there is a great potential for\nfurther research."
    },
    {
      "id": "2411.09955v2",
      "title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era",
      "author": "Thanh Tam Nguyen",
      "coauthors": [
        "Zhao Ren",
        "Trinh Pham",
        "Thanh Trung Huynh",
        "Phi Le Nguyen",
        "Hongzhi Yin",
        "Quoc Viet Hung Nguyen"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing."
    },
    {
      "id": "2411.13851v1",
      "title": "Arm Robot: AR-Enhanced Embodied Control and Visualization for Intuitive\n  Robot Arm Manipulation",
      "author": "Siyou Pei",
      "coauthors": [
        "Alexander Chen",
        "Ronak Kaoshik",
        "Ruofei Du",
        "Yang Zhang"
      ],
      "abstract": "Embodied interaction has been introduced to human-robot interaction (HRI) as\na type of teleoperation, in which users control robot arms with bodily action\nvia handheld controllers or haptic gloves. Embodied teleoperation has made\nrobot control intuitive to non-technical users, but differences between humans'\nand robots' capabilities \\eg ranges of motion and response time, remain\nchallenging. In response, we present Arm Robot, an embodied robot arm\nteleoperation system that helps users tackle human-robot discrepancies.\nSpecifically, Arm Robot (1) includes AR visualization as real-time feedback on\ntemporal and spatial discrepancies, and (2) allows users to change observing\nperspectives and expand action space. We conducted a user study (N=18) to\ninvestigate the usability of the Arm Robot and learn how users perceive the\nembodiment. Our results show users could use Arm Robot's features to\neffectively control the robot arm, providing insights for continued work in\nembodied HRI."
    },
    {
      "id": "2411.13965v1",
      "title": "Does the square-root price impact law belong to the strict universal\n  scalings?: quantitative support by a complete survey of the Tokyo stock\n  exchange market",
      "author": "Yuki Sato",
      "coauthors": [
        "Kiyoshi Kanazawa"
      ],
      "abstract": "Universal power laws have been scrutinised in physics and beyond, and a\nlong-standing debate exists in econophysics regarding the strict universality\nof the nonlinear price impact, commonly referred to as the square-root law\n(SRL). The SRL posits that the average price impact $I$ follows a power law\nwith respect to transaction volume $Q$, such that $I(Q) \\propto Q^{\\delta}$\nwith $\\delta \\approx 1/2$. Some researchers argue that the exponent $\\delta$\nshould be system-specific, without universality. Conversely, others contend\nthat $\\delta$ should be exactly $1/2$ for all stocks across all countries,\nimplying universality. However, resolving this debate requires high-precision\nmeasurements of $\\delta$ with errors of around $0.1$ across hundreds of stocks,\nwhich has been extremely challenging due to the scarcity of large microscopic\ndatasets -- those that enable tracking the trading behaviour of all individual\naccounts. Here we conclusively support the universality hypothesis of the SRL\nby a complete survey of all trading accounts for all liquid stocks on the Tokyo\nStock Exchange (TSE) over eight years. Using this comprehensive microscopic\ndataset, we show that the exponent $\\delta$ is equal to $1/2$ within\nstatistical errors at both the individual stock level and the individual trader\nlevel. Additionally, we rejected two prominent models supporting the\nnonuniversality hypothesis: the Gabaix-Gopikrishnan-Plerou-Stanley and the\nFarmer-Gerig-Lillo-Waelbroeck models. Our work provides exceptionally\nhigh-precision evidence for the universality hypothesis in social science and\ncould prove useful in evaluating the price impact by large investors -- an\nimportant topic even among practitioners."
    },
    {
      "id": "2411.13792v1",
      "title": "Multiscale Markowitz",
      "author": "Revant Nayar",
      "coauthors": [
        "Raphael Douady"
      ],
      "abstract": "Traditional Markowitz portfolio optimization constrains daily portfolio\nvariance to a target value, optimising returns, Sharpe or variance within this\nconstraint. However, this approach overlooks the relationship between variance\nat different time scales, typically described by $\\sigma(\\Delta t) \\propto\n(\\Delta t)^{H}$ where $H$ is the Hurst exponent, most of the time assumed to be\n\\(\\frac{1}{2}\\). This paper introduces a multifrequency optimization framework\nthat allows investors to specify target portfolio variance across a range of\nfrequencies, characterized by a target Hurst exponent $H_{target}$, or optimize\nthe portfolio at multiple time scales. By incorporating this scaling behavior,\nwe enable a more nuanced and comprehensive risk management strategy that aligns\nwith investor preferences at various time scales. This approach effectively\nmanages portfolio risk across multiple frequencies and adapts to different\nmarket conditions, providing a robust tool for dynamic asset allocation. This\novercomes some of the traditional limitations of Markowitz, when it comes to\ndealing with crashes, regime changes, volatility clustering or multifractality\nin markets. We illustrate this concept with a toy example and discuss the\npractical implementation for assets with varying scaling behaviors."
    },
    {
      "id": "2409.10301v2",
      "title": "Decomposition Pipeline for Large-Scale Portfolio Optimization with\n  Applications to Near-Term Quantum Computing",
      "author": "Atithi Acharya",
      "coauthors": [
        "Romina Yalovetzky",
        "Pierre Minssen",
        "Shouvanik Chakrabarti",
        "Ruslan Shaydulin",
        "Rudy Raymond",
        "Yue Sun",
        "Dylan Herman",
        "Ruben S. Andrist",
        "Grant Salton",
        "Martin J. A. Schuetz",
        "Helmut G. Katzgraber",
        "Marco Pistoia"
      ],
      "abstract": "Industrially relevant constrained optimization problems, such as portfolio\noptimization and portfolio rebalancing, are often intractable or difficult to\nsolve exactly. In this work, we propose and benchmark a decomposition pipeline\ntargeting portfolio optimization and rebalancing problems with constraints. The\npipeline decomposes the optimization problem into constrained subproblems,\nwhich are then solved separately and aggregated to give a final result. Our\npipeline includes three main components: preprocessing of correlation matrices\nbased on random matrix theory, modified spectral clustering based on Newman's\nalgorithm, and risk rebalancing. Our empirical results show that our pipeline\nconsistently decomposes real-world portfolio optimization problems into\nsubproblems with a size reduction of approximately 80%. Since subproblems are\nthen solved independently, our pipeline drastically reduces the total\ncomputation time for state-of-the-art solvers. Moreover, by decomposing large\nproblems into several smaller subproblems, the pipeline enables the use of\nnear-term quantum devices as solvers, providing a path toward practical utility\nof quantum computers in portfolio optimization."
    },
    {
      "id": "2411.13180v1",
      "title": "Investor Sentiment in Asset Pricing Models: A Review of Empirical\n  Evidence",
      "author": "Szymon Lis",
      "coauthors": [],
      "abstract": "This study conducted a comprehensive review of 71 papers published between\n2000 and 2021 that employed various measures of investor sentiment to model\nreturns. The analysis indicates that higher complexity of sentiment measures\nand models improves the coefficient of determination. However, there was\ninsufficient evidence to support that models incorporating more complex\nsentiment measures have better predictive power than those employing simpler\nproxies. Additionally, the significance of sentiment varies based on the asset\nand time period being analyzed, suggesting that the consensus relying on the BW\nindex as a sentiment measure may be subject to change."
    },
    {
      "id": "2411.12323v1",
      "title": "Mirror Descent Algorithms for Risk Budgeting Portfolios",
      "author": "Martin Arnaiz Iglesias",
      "coauthors": [
        "Adil Rengim Cetingoz",
        "Noufel Frikha"
      ],
      "abstract": "This paper introduces and examines numerical approximation schemes for\ncomputing risk budgeting portfolios associated to positive homogeneous and\nsub-additive risk measures. We employ Mirror Descent algorithms to determine\nthe optimal risk budgeting weights in both deterministic and stochastic\nsettings, establishing convergence along with an explicit non-asymptotic\nquantitative rate for the averaged algorithm. A comprehensive numerical\nanalysis follows, illustrating our theoretical findings across various risk\nmeasures -- including standard deviation, Expected Shortfall, deviation\nmeasures, and Variantiles -- and comparing the performance with that of the\nstandard stochastic gradient descent method recently proposed in the\nliterature."
    },
    {
      "id": "2411.08864v2",
      "title": "Isotropic Correlation Models for the Cross-Section of Equity Returns",
      "author": "Graham L. Giller",
      "coauthors": [],
      "abstract": "This note discusses some of the aspects of a model for the covariance of\nequity returns based on a simple \"isotropic\" structure in which all pairwise\ncorrelations are taken to be the same value. The effect of the structure on\nfeasible values for the common correlation of returns and on the \"effective\ndegrees of freedom\" within the equity cross-section are discussed, as well as\nthe impact of this constraint on the asymptotic Normality of portfolio returns.\nAn eigendecomposition of the covariance matrix is presented and used to\npartition variance into that from a common \"market\" factor and\n\"non-diversifiable\" idiosyncratic risk. A empirical analysis of the recent\nhistory of the returns of S&P 500 Index members is presented and compared to\nthe expectations from both this model and linear factor models. This analysis\nsupports the isotropic covariance model and does not seem to provide evidence\nin support of linear factor models. Analysis of portfolio selection under\nisotropic correlation is presented using mean-variance optimization for both\nheteroskedastic and homoskedastic cases. Portfolio selection for negative\nexponential utility maximizers is also discussed for the general case of\ndistributions of returns with elliptical symmetry. The fact that idiosyncratic\nrisk may not be removed by diversification in a model that the data supports\nundermines the basic premises of structures such as the C.A.P.M. and A.P.T. If\nthe cross-section of equity returns is more accurately described by this\nstructure then an inevitable consequence is that picking stocks is not a\n\"pointless\" activity, as the returns to residual risk would be non-zero."
    },
    {
      "id": "2308.02820v2",
      "title": "Reinforcement Learning for Financial Index Tracking",
      "author": "Xianhua Peng",
      "coauthors": [
        "Chenyin Gong",
        "Xue Dong He"
      ],
      "abstract": "We propose the first discrete-time infinite-horizon dynamic formulation of\nthe financial index tracking problem under both return-based tracking error and\nvalue-based tracking error. The formulation overcomes the limitations of\nexisting models by incorporating the intertemporal dynamics of market\ninformation variables not limited to prices, allowing exact calculation of\ntransaction costs, accounting for the tradeoff between overall tracking error\nand transaction costs, allowing effective use of data in a long time period,\netc. The formulation also allows novel decision variables of cash injection or\nwithdraw. We propose to solve the portfolio rebalancing equation using a Banach\nfixed point iteration, which allows to accurately calculate the transaction\ncosts specified as nonlinear functions of trading volumes in practice. We\npropose an extension of deep reinforcement learning (RL) method to solve the\ndynamic formulation. Our RL method resolves the issue of data limitation\nresulting from the availability of a single sample path of financial data by a\nnovel training scheme. A comprehensive empirical study based on a 17-year-long\ntesting set demonstrates that the proposed method outperforms a benchmark\nmethod in terms of tracking accuracy and has the potential for earning extra\nprofit through cash withdraw strategy."
    },
    {
      "id": "2410.19030v3",
      "title": "Loss Aversion and State-Dependent Linear Utility Functions for Monetary\n  Returns",
      "author": "Somdeb Lahiri",
      "coauthors": [],
      "abstract": "We present a theory of expected utility with state-dependent linear utility\nfunctions for monetary returns, that incorporates the possibility of\nloss-aversion. Our results relate to first order stochastic dominance,\nmean-preserving spread, increasing-concave linear utility profiles and risk\naversion. As an application of the expected utility theory developed here, we\nanalyze the contract that a monopolist would offer in an insurance market that\nallowed for partial coverage of loss."
    },
    {
      "id": "2404.18467v2",
      "title": "Dominance between combinations of infinite-mean Pareto random variables",
      "author": "Yuyu Chen",
      "coauthors": [
        "Taizhong Hu",
        "Ruodu Wang",
        "Zhenfeng Zou"
      ],
      "abstract": "We study stochastic dominance between portfolios of independent and\nidentically distributed (iid) extremely heavy-tailed (i.e., infinite-mean)\nPareto random variables. With the notion of majorization order, we show that a\nmore diversified portfolio of iid extremely heavy-tailed Pareto random\nvariables is larger in the sense of first-order stochastic dominance. This\nresult is further generalized for Pareto random variables caused by triggering\nevents, random variables with tails being Pareto, bounded Pareto random\nvariables, and positively dependent Pareto random variables. These results\nprovide an important implication in investment: Diversification of extremely\nheavy-tailed Pareto profits uniformly increases investors' profitability,\nleading to a diversification benefit. Remarkably, different from the\nfinite-mean setting, such a diversification benefit does not depend on the\ndecision maker's risk aversion."
    },
    {
      "id": "2110.09516v2",
      "title": "Keep it Tighter -- A Story on Analytical Mean Embeddings",
      "author": "Linda Chamakh",
      "coauthors": [
        "Zoltan Szabo"
      ],
      "abstract": "Kernel techniques are among the most popular and flexible approaches in data\nscience allowing to represent probability measures without loss of information\nunder mild conditions. The resulting mapping called mean embedding gives rise\nto a divergence measure referred to as maximum mean discrepancy (MMD) with\nexisting quadratic-time estimators (w.r.t. the sample size) and known\nconvergence properties for bounded kernels. In this paper we focus on the\nproblem of MMD estimation when the mean embedding of one of the underlying\ndistributions is available analytically. Particularly, we consider\ndistributions on the real line (motivated by financial applications) and prove\ntighter concentration for the proposed estimator under this semi-explicit\nsetting; we also extend the result to the case of unbounded (exponential)\nkernel with minimax-optimal lower bounds. We demonstrate the efficiency of our\napproach beyond synthetic example in three real-world examples relying on\none-dimensional random variables: index replication and calibration on\nloss-given-default ratios and on S&P 500 data."
    },
    {
      "id": "2411.14068v1",
      "title": "Calculating Profits and Losses for Algorithmic Trading Strategies: A\n  Short Guide",
      "author": "James B. Glattfelder",
      "coauthors": [
        "Thomas Houweling"
      ],
      "abstract": "We present a series of equations that track the total realized and unrealized\nprofits and losses at any time, incorporating the spread. The resulting\nformalism is ideally suited to evaluate the performance of trading model\nalgorithms."
    },
    {
      "id": "2411.13965v1",
      "title": "Does the square-root price impact law belong to the strict universal\n  scalings?: quantitative support by a complete survey of the Tokyo stock\n  exchange market",
      "author": "Yuki Sato",
      "coauthors": [
        "Kiyoshi Kanazawa"
      ],
      "abstract": "Universal power laws have been scrutinised in physics and beyond, and a\nlong-standing debate exists in econophysics regarding the strict universality\nof the nonlinear price impact, commonly referred to as the square-root law\n(SRL). The SRL posits that the average price impact $I$ follows a power law\nwith respect to transaction volume $Q$, such that $I(Q) \\propto Q^{\\delta}$\nwith $\\delta \\approx 1/2$. Some researchers argue that the exponent $\\delta$\nshould be system-specific, without universality. Conversely, others contend\nthat $\\delta$ should be exactly $1/2$ for all stocks across all countries,\nimplying universality. However, resolving this debate requires high-precision\nmeasurements of $\\delta$ with errors of around $0.1$ across hundreds of stocks,\nwhich has been extremely challenging due to the scarcity of large microscopic\ndatasets -- those that enable tracking the trading behaviour of all individual\naccounts. Here we conclusively support the universality hypothesis of the SRL\nby a complete survey of all trading accounts for all liquid stocks on the Tokyo\nStock Exchange (TSE) over eight years. Using this comprehensive microscopic\ndataset, we show that the exponent $\\delta$ is equal to $1/2$ within\nstatistical errors at both the individual stock level and the individual trader\nlevel. Additionally, we rejected two prominent models supporting the\nnonuniversality hypothesis: the Gabaix-Gopikrishnan-Plerou-Stanley and the\nFarmer-Gerig-Lillo-Waelbroeck models. Our work provides exceptionally\nhigh-precision evidence for the universality hypothesis in social science and\ncould prove useful in evaluating the price impact by large investors -- an\nimportant topic even among practitioners."
    },
    {
      "id": "2411.13381v1",
      "title": "Simulating Liquidity: Agent-Based Modeling of Illiquid Markets for\n  Fractional Ownership",
      "author": "Lars Fluri",
      "coauthors": [
        "A. Ege Yilmaz",
        "Denis Bieri",
        "Thomas Ankenbrand",
        "Aurelio Perucca"
      ],
      "abstract": "This research investigates liquidity dynamics in fractional ownership\nmarkets, focusing on illiquid alternative investments traded on a FinTech\nplatform. By leveraging empirical data and employing agent-based modeling\n(ABM), the study simulates trading behaviors in sell offer-driven systems,\nproviding a foundation for generating insights into how different market\nstructures influence liquidity. The ABM-based simulation model provides a data\naugmentation environment which allows for the exploration of diverse trading\narchitectures and rules, offering an alternative to direct experimentation.\nThis approach bridges academic theory and practical application, supported by\ncollaboration with industry and Swiss federal funding. The paper lays the\nfoundation for planned extensions, including the identification of a\nliquidity-maximizing trading environment and the design of a market maker, by\nsimulating the current functioning of the investment platform using an ABM\nspecified with empirical data."
    },
    {
      "id": "2402.01441v2",
      "title": "Learning the Market: Sentiment-Based Ensemble Trading Agents",
      "author": "Andrew Ye",
      "coauthors": [
        "James Xu",
        "Vidyut Veedgav",
        "Yi Wang",
        "Yifan Yu",
        "Daniel Yan",
        "Ryan Chen",
        "Vipin Chaudhary",
        "Shuai Xu"
      ],
      "abstract": "We propose and study the integration of sentiment analysis and deep\nreinforcement learning ensemble algorithms for stock trading by evaluating\nstrategies capable of dynamically altering their active agent given the\nconcurrent market environment. In particular, we design a simple-yet-effective\nmethod for extracting financial sentiment and combine this with improvements on\nexisting trading agents, resulting in a strategy that effectively considers\nboth qualitative market factors and quantitative stock data. We show that our\napproach results in a strategy that is profitable, robust, and risk-minimal -\noutperforming the traditional ensemble strategy as well as single agent\nalgorithms and market metrics. Our findings suggest that the conventional\npractice of switching and reevaluating agents in ensemble every fixed-number of\nmonths is sub-optimal, and that a dynamic sentiment-based framework greatly\nunlocks additional performance. Furthermore, as we have designed our algorithm\nwith simplicity and efficiency in mind, we hypothesize that the transition of\nour method from historical evaluation towards real-time trading with live data\nto be relatively simple."
    },
    {
      "id": "2411.13594v1",
      "title": "High resolution microprice estimates from limit orderbook data using\n  hyperdimensional vector Tsetlin Machines",
      "author": "Christian D. Blakely",
      "coauthors": [],
      "abstract": "We propose an error-correcting model for the microprice, a high-frequency\nestimator of future prices given higher order information of imbalances in the\norderbook. The model takes into account a current microprice estimate given the\nspread and best bid to ask imbalance, and adjusts the microprice based on\nrecent dynamics of higher price rank imbalances. We introduce a computationally\nfast estimator using a recently proposed hyperdimensional vector Tsetlin\nmachine framework and demonstrate empirically that this estimator can provide a\nrobust estimate of future prices in the orderbook."
    },
    {
      "id": "2411.08804v1",
      "title": "FinRobot: AI Agent for Equity Research and Valuation with Large Language\n  Models",
      "author": "Tianyu Zhou",
      "coauthors": [
        "Pinqiao Wang",
        "Yilin Wu",
        "Hongyang Yang"
      ],
      "abstract": "As financial markets grow increasingly complex, there is a rising need for\nautomated tools that can effectively assist human analysts in equity research,\nparticularly within sell-side research. While Generative AI (GenAI) has\nattracted significant attention in this field, existing AI solutions often fall\nshort due to their narrow focus on technical factors and limited capacity for\ndiscretionary judgment. These limitations hinder their ability to adapt to new\ndata in real-time and accurately assess risks, which diminishes their practical\nvalue for investors.\n  This paper presents FinRobot, the first AI agent framework specifically\ndesigned for equity research. FinRobot employs a multi-agent Chain of Thought\n(CoT) system, integrating both quantitative and qualitative analyses to emulate\nthe comprehensive reasoning of a human analyst. The system is structured around\nthree specialized agents: the Data-CoT Agent, which aggregates diverse data\nsources for robust financial integration; the Concept-CoT Agent, which mimics\nan analysts reasoning to generate actionable insights; and the Thesis-CoT\nAgent, which synthesizes these insights into a coherent investment thesis and\nreport. FinRobot provides thorough company analysis supported by precise\nnumerical data, industry-appropriate valuation metrics, and realistic risk\nassessments. Its dynamically updatable data pipeline ensures that research\nremains timely and relevant, adapting seamlessly to new financial information.\nUnlike existing automated research tools, such as CapitalCube and Wright\nReports, FinRobot delivers insights comparable to those produced by major\nbrokerage firms and fundamental research vendors. We open-source FinRobot at\n\\url{https://github. com/AI4Finance-Foundation/FinRobot}."
    },
    {
      "id": "2411.08637v1",
      "title": "Robot See, Robot Do: Imitation Reward for Noisy Financial Environments",
      "author": "Sven Golu\u017ea",
      "coauthors": [
        "Tomislav Kova\u010devi\u0107",
        "Stjepan Begu\u0161i\u0107",
        "Zvonko Kostanj\u010dar"
      ],
      "abstract": "The sequential nature of decision-making in financial asset trading aligns\nnaturally with the reinforcement learning (RL) framework, making RL a common\napproach in this domain. However, the low signal-to-noise ratio in financial\nmarkets results in noisy estimates of environment components, including the\nreward function, which hinders effective policy learning by RL agents. Given\nthe critical importance of reward function design in RL problems, this paper\nintroduces a novel and more robust reward function by leveraging imitation\nlearning, where a trend labeling algorithm acts as an expert. We integrate\nimitation (expert's) feedback with reinforcement (agent's) feedback in a\nmodel-free RL algorithm, effectively embedding the imitation learning problem\nwithin the RL paradigm to handle the stochasticity of reward signals. Empirical\nresults demonstrate that this novel approach improves financial performance\nmetrics compared to traditional benchmarks and RL agents trained solely using\nreinforcement feedback."
    },
    {
      "id": "2411.08382v1",
      "title": "Hybrid Vector Auto Regression and Neural Network Model for Order Flow\n  Imbalance Prediction in High Frequency Trading",
      "author": "Abdul Rahman",
      "coauthors": [
        "Neelesh Upadhye"
      ],
      "abstract": "In high frequency trading, accurate prediction of Order Flow Imbalance (OFI)\nis crucial for understanding market dynamics and maintaining liquidity. This\npaper introduces a hybrid predictive model that combines Vector Auto Regression\n(VAR) with a simple feedforward neural network (FNN) to forecast OFI and assess\ntrading intensity. The VAR component captures linear dependencies, while\nresiduals are fed into the FNN to model non-linear patterns, enabling a\ncomprehensive approach to OFI prediction. Additionally, the model calculates\nthe intensity on the Buy or Sell side, providing insights into which side holds\ngreater trading pressure. These insights facilitate the development of trading\nstrategies by identifying periods of high buy or sell intensity. Using both\nsynthetic and real trading data from Binance, we demonstrate that the hybrid\nmodel offers significant improvements in predictive accuracy and enhances\nstrategic decision-making based on OFI dynamics. Furthermore, we compare the\nhybrid models performance with standalone FNN and VAR models, showing that the\nhybrid approach achieves superior forecasting accuracy across both synthetic\nand real datasets, making it the most effective model for OFI prediction in\nhigh frequency trading."
    },
    {
      "id": "2411.08145v1",
      "title": "Automated Market Making: the case of Pegged Assets",
      "author": "Philippe Bergault",
      "coauthors": [
        "Louis Bertucci",
        "David Bouba",
        "Olivier Gu\u00e9ant",
        "Julien Guilbert"
      ],
      "abstract": "In this paper, we introduce a novel framework to model the exchange rate\ndynamics between two intrinsically linked cryptoassets, such as stablecoins\npegged to the same fiat currency or a liquid staking token and its associated\nnative token. Our approach employs multi-level nested Ornstein-Uhlenbeck (OU)\nprocesses, for which we derive key properties and develop calibration and\nfiltering techniques. Then, we design an automated market maker (AMM) model\nspecifically tailored for the swapping of closely related cryptoassets.\nDistinct from existing models, our AMM leverages the unique exchange rate\ndynamics provided by the multi-level nested OU processes, enabling more precise\nrisk management and enhanced liquidity provision. We validate the model through\nnumerical simulations using real-world data for the USDC/USDT and wstETH/WETH\npairs, demonstrating that it consistently yields efficient quotes. This\napproach offers significant potential to improve liquidity in markets for\npegged assets."
    },
    {
      "id": "2411.07732v1",
      "title": "Implementing Dynamic Pricing Across Multiple Pricing Groups in Real\n  Estate",
      "author": "Lev Razumovskiy",
      "coauthors": [
        "Mariya Gerasimova",
        "Nikolay Karenin",
        "Mikhail Safro"
      ],
      "abstract": "This article presents a mathematical model of dynamic pricing for real estate\n(RE) that incorporates multiple pricing groups, thereby expanding the\ncapabilities of existing models. The developed model solves the problem of\nmaximizing aggregate cumulative revenue at the end of the sales period while\nmeeting the revenue and sales goals. A method is proposed for distributing\naggregate cumulative revenue goals across different RE pricing groups. The\nmodel is further modified to account for the time value of money and the real\nestate value increase as construction progresses. The algorithm for\nconstructing a pricing policy for multiple pricing groups is described, and\nnumerical simulations are performed to demonstrate how the algorithm operates."
    },
    {
      "id": "2411.13965v1",
      "title": "Does the square-root price impact law belong to the strict universal\n  scalings?: quantitative support by a complete survey of the Tokyo stock\n  exchange market",
      "author": "Yuki Sato",
      "coauthors": [
        "Kiyoshi Kanazawa"
      ],
      "abstract": "Universal power laws have been scrutinised in physics and beyond, and a\nlong-standing debate exists in econophysics regarding the strict universality\nof the nonlinear price impact, commonly referred to as the square-root law\n(SRL). The SRL posits that the average price impact $I$ follows a power law\nwith respect to transaction volume $Q$, such that $I(Q) \\propto Q^{\\delta}$\nwith $\\delta \\approx 1/2$. Some researchers argue that the exponent $\\delta$\nshould be system-specific, without universality. Conversely, others contend\nthat $\\delta$ should be exactly $1/2$ for all stocks across all countries,\nimplying universality. However, resolving this debate requires high-precision\nmeasurements of $\\delta$ with errors of around $0.1$ across hundreds of stocks,\nwhich has been extremely challenging due to the scarcity of large microscopic\ndatasets -- those that enable tracking the trading behaviour of all individual\naccounts. Here we conclusively support the universality hypothesis of the SRL\nby a complete survey of all trading accounts for all liquid stocks on the Tokyo\nStock Exchange (TSE) over eight years. Using this comprehensive microscopic\ndataset, we show that the exponent $\\delta$ is equal to $1/2$ within\nstatistical errors at both the individual stock level and the individual trader\nlevel. Additionally, we rejected two prominent models supporting the\nnonuniversality hypothesis: the Gabaix-Gopikrishnan-Plerou-Stanley and the\nFarmer-Gerig-Lillo-Waelbroeck models. Our work provides exceptionally\nhigh-precision evidence for the universality hypothesis in social science and\ncould prove useful in evaluating the price impact by large investors -- an\nimportant topic even among practitioners."
    },
    {
      "id": "2411.13762v1",
      "title": "Assessing Stablecoin Credit Risks",
      "author": "Yuval Boneh",
      "coauthors": [
        "Ethan Jones"
      ],
      "abstract": "This paper delves into the spectrum of credit risks associated with\ndecentralized stablecoin issuance, ranging from overcollateralized lending to\nbusiness-to-business credit. It examines the mechanisms, risks, and mitigation\nstrategies at each layer, highlighting the potential for scaling decentralized\nstablecoins while ensuring systemic health."
    },
    {
      "id": "2409.10301v2",
      "title": "Decomposition Pipeline for Large-Scale Portfolio Optimization with\n  Applications to Near-Term Quantum Computing",
      "author": "Atithi Acharya",
      "coauthors": [
        "Romina Yalovetzky",
        "Pierre Minssen",
        "Shouvanik Chakrabarti",
        "Ruslan Shaydulin",
        "Rudy Raymond",
        "Yue Sun",
        "Dylan Herman",
        "Ruben S. Andrist",
        "Grant Salton",
        "Martin J. A. Schuetz",
        "Helmut G. Katzgraber",
        "Marco Pistoia"
      ],
      "abstract": "Industrially relevant constrained optimization problems, such as portfolio\noptimization and portfolio rebalancing, are often intractable or difficult to\nsolve exactly. In this work, we propose and benchmark a decomposition pipeline\ntargeting portfolio optimization and rebalancing problems with constraints. The\npipeline decomposes the optimization problem into constrained subproblems,\nwhich are then solved separately and aggregated to give a final result. Our\npipeline includes three main components: preprocessing of correlation matrices\nbased on random matrix theory, modified spectral clustering based on Newman's\nalgorithm, and risk rebalancing. Our empirical results show that our pipeline\nconsistently decomposes real-world portfolio optimization problems into\nsubproblems with a size reduction of approximately 80%. Since subproblems are\nthen solved independently, our pipeline drastically reduces the total\ncomputation time for state-of-the-art solvers. Moreover, by decomposing large\nproblems into several smaller subproblems, the pipeline enables the use of\nnear-term quantum devices as solvers, providing a path toward practical utility\nof quantum computers in portfolio optimization."
    },
    {
      "id": "2411.13384v1",
      "title": "Comparisons of multivariate contribution measures of risk contagion and\n  their applications in cryptocurrency market",
      "author": "Limin Wen",
      "coauthors": [
        "Junxue Li",
        "Tong Pu",
        "Yiying Zhang"
      ],
      "abstract": "Conditional risk measures and their associated risk contribution measures are\ncommonly employed in finance and actuarial science for evaluating systemic risk\nand quantifying the effects of risk contagion. This paper introduces various\ntypes of contribution measures based on the MCoVaR, MCoES, and MMME studied in\nOrtega-Jim\\'enez et al. (2021) and Das & Fasen-Hartmann (2018) to assess both\nthe absolute and relative effects of a single risk when other risks in a group\nare in distress. The properties of these contribution risk measures are\nexamined, and sufficient conditions for comparing these measures between two\nsets of random vectors are established using univariate and multivariate\nstochastic orders and stochastic dependence notions. Numerical examples are\npresented for validating the conditions. Finally, a real dataset from the\ncryptocurrency market is also utilized to analyze the contagion effect in terms\nof our proposed contribution measures."
    },
    {
      "id": "2407.09321v2",
      "title": "A note on Skew Brownian Motion with two-valued drift and an application",
      "author": "Zaniar Ahmadi",
      "coauthors": [
        "Xiaowen Zhou"
      ],
      "abstract": "For skew Brownian motion with two-valued drift, adopting a perturbation\napproach we find expressions of its potential densities. As applications, we\nrecover its transition density and study its long-time asymptotic behaviors. We\nalso compare with previous results on transition densities for skew Brownian\nmotions. We propose two approaches for generating quasi-random samples by\napproximating the cumulative distribution function and discussing their risk\nmeasurement application."
    },
    {
      "id": "2303.08565v2",
      "title": "Probabilistic forecasting with a hybrid Factor-QRA approach: Application\n  to electricity trading",
      "author": "Katarzyna Maciejowska",
      "coauthors": [
        "Tomasz Serafin",
        "Bartosz Uniejewski"
      ],
      "abstract": "This paper presents a novel hybrid approach for constricting probabilistic\nforecasts that combines both the Quantile Regression Averaging (QRA) method and\nthe factor-based averaging scheme. The performance of the approach is evaluated\non data sets from two European energy markets - the German EPEX SPOT and the\nPolish Power Exchange (TGE). The results show that the newly proposed method\noutperforms literature benchmarks in terms of statistical measures: the\nempirical coverage and the Christoffersen test for conditional coverage.\nMoreover, in line with recent literature trends, the economic value of\nforecasts is evaluated based on the trading strategy using probabilistic price\npredictions to optimize the operation of an energy storage system. The results\nsuggest that apart from the use of statistical measures, there is a need for\nthe economic evaluation of forecasts."
    },
    {
      "id": "2411.12522v1",
      "title": "Canonical insurance models: stochastic equations and comparison theorems",
      "author": "Marcus C. Christiansen",
      "coauthors": [
        "Christian Furrer"
      ],
      "abstract": "Thiele's differential equation explains the change in prospective reserve and\nplays a fundamental role in safe-side calculations and other types of actuarial\nmodel comparisons. This paper presents a `model lean' version of Thiele's\nequation with the novel feature that it supports any canonical insurance model,\nirrespective of the model's intertemporal dependence structure. The basis for\nthis is a canonical and path-wise model construction that simultaneously\nhandles discrete and absolutely continuous modeling regimes. Comparison\ntheorems for differing canonical insurance models follow directly from the\nresulting stochastic backward equations. The elegance with which these\ncomparison theorems handle non-equivalence of probability measures is one of\ntheir major advantages over previous results."
    },
    {
      "id": "2411.12323v1",
      "title": "Mirror Descent Algorithms for Risk Budgeting Portfolios",
      "author": "Martin Arnaiz Iglesias",
      "coauthors": [
        "Adil Rengim Cetingoz",
        "Noufel Frikha"
      ],
      "abstract": "This paper introduces and examines numerical approximation schemes for\ncomputing risk budgeting portfolios associated to positive homogeneous and\nsub-additive risk measures. We employ Mirror Descent algorithms to determine\nthe optimal risk budgeting weights in both deterministic and stochastic\nsettings, establishing convergence along with an explicit non-asymptotic\nquantitative rate for the averaged algorithm. A comprehensive numerical\nanalysis follows, illustrating our theoretical findings across various risk\nmeasures -- including standard deviation, Expected Shortfall, deviation\nmeasures, and Variantiles -- and comparing the performance with that of the\nstandard stochastic gradient descent method recently proposed in the\nliterature."
    },
    {
      "id": "2411.11522v1",
      "title": "Robust Bernoulli mixture models for credit portfolio risk",
      "author": "Jonathan Ansari",
      "coauthors": [
        "Eva L\u00fctkebohmert"
      ],
      "abstract": "This paper presents comparison results and establishes risk bounds for credit\nportfolios within classes of Bernoulli mixture models, assuming conditionally\nindependent defaults that are stochastically increasing with a common risk\nfactor. We provide simple and interpretable conditions for conditional default\nprobabilities that imply a comparison of credit portfolio losses in convex\norder. In the case of threshold models, the ranking of portfolio losses is\nbased on a pointwise comparison of the underlying copulas. Our setting includes\nas special case the well-known Gaussian copula model but allows for general\ntail dependencies, which are crucial for modeling credit portfolio risks.\nMoreover, our results extend the classical parameterized models, such as the\nindustry models CreditMetrics and KMV Portfolio Manager, to a robust setting\nwhere individual parameters or the copula modeling the dependence structure can\nbe ambiguous. A simulation study and a real data example under model\nuncertainty offer evidence supporting the effectiveness of our approach."
    },
    {
      "id": "2403.16525v4",
      "title": "Measuring Name Concentrations through Deep Learning",
      "author": "Eva L\u00fctkebohmert",
      "coauthors": [
        "Julian Sester"
      ],
      "abstract": "We propose a new deep learning approach for the quantification of name\nconcentration risk in loan portfolios. Our approach is tailored for small\nportfolios and allows for both an actuarial as well as a mark-to-market\ndefinition of loss. The training of our neural network relies on Monte Carlo\nsimulations with importance sampling which we explicitly formulate for the\nCreditRisk${+}$ and the ratings-based CreditMetrics model. Numerical results\nbased on simulated as well as real data demonstrate the accuracy of our new\napproach and its superior performance compared to existing analytical methods\nfor assessing name concentration risk in small and concentrated portfolios."
    },
    {
      "id": "2411.13810v1",
      "title": "Dynamic spatial interaction models for a leader's resource allocation\n  and followers' multiple activities",
      "author": "Hanbat Jeong",
      "coauthors": [],
      "abstract": "This paper introduces a novel spatial interaction model to explore the\ndecision-making processes of two types of agents-a leader and followers-with\ncentral and local governments serving as empirical representations. The model\naccounts for three key features: (i) resource allocations from the leader to\nthe followers and the resulting strategic interactions, (ii) followers' choices\nacross multiple activities, and (iii) interactions among these activities. We\ndevelop a network game to examine the micro-foundations of these processes. In\nthis game, followers engage in multiple activities, while the leader allocates\nresources by monitoring the externalities arising from followers' interactions.\nThe game's unique NE is the foundation for our econometric framework, providing\nequilibrium measures to understand the short-term impacts of changes in\nfollowers' characteristics and their long-term consequences. To estimate the\nagent payoff parameters, we employ the QML estimation method and examine the\nasymptotic properties of the QML estimator to ensure robust statistical\ninferences. Empirically, we investigate interactions among U.S. states in\npublic welfare expenditures (PWE) and housing and community development\nexpenditures (HCDE), focusing on how federal grants influence these\nexpenditures and the interactions among state governments. Our findings reveal\npositive spillovers in states' PWEs, complementarity between the two\nexpenditures within states, and negative cross-variable spillovers between\nthem. Additionally, we observe positive effects of federal grants on both\nexpenditures. Counterfactual simulations indicate that federal interventions\nlead to a 6.46% increase in social welfare by increasing the states' efforts on\nPWE and HCDE. However, due to the limited flexibility in federal grants, their\nmagnitudes are smaller than the proportion of federal grants within the states'\ntotal revenues."
    },
    {
      "id": "2403.03299v3",
      "title": "Demystifying and avoiding the OLS \"weighting problem\": Unmodeled\n  heterogeneity and straightforward solutions",
      "author": "Chad Hazlett",
      "coauthors": [
        "Tanvi Shinkre"
      ],
      "abstract": "Researchers have long run regressions of an outcome variable (Y) on a\ntreatment (D) and covariates (X) to estimate treatment effects. Even absent\nunobserved confounding, the regression coefficient on D in this setup reports a\nconditional variance weighted average of strata-wise average effects, not\ngenerally equal to the average treatment effect (ATE). Numerous proposals have\nbeen offered to cope with this \"weighting problem\", including interpretational\ntools to help characterize the weights and diagnostic aids to help researchers\nassess the potential severity of this problem. We make two contributions that\ntogether suggest an alternative direction for researchers and this literature.\nOur first contribution is conceptual, demystifying these weights. Simply put,\nunder heterogeneous treatment effects (and varying probability of treatment),\nthe linear regression of Y on D and X will be misspecified. The \"weights\" of\nregression offer one characterization for the coefficient from regression that\nhelps to clarify how it will depart from the ATE. We also derive a more general\nexpression for the weights than what is usually referenced. Our second\ncontribution is practical: as these weights simply characterize\nmisspecification bias, we suggest simply avoiding them through an approach that\ntolerate heterogeneous effects. A wide range of longstanding alternatives\n(regression-imputation/g-computation, interacted regression, and balancing\nweights) relax specification assumptions to allow heterogeneous effects. We\nmake explicit the assumption of \"separate linearity\", under which each\npotential outcome is separately linear in X. This relaxation of conventional\nlinearity offers a common justification for all of these methods and avoids the\nweighting problem, at an efficiency cost that will be small when there are few\ncovariates relative to sample size."
    },
    {
      "id": "2307.01284v4",
      "title": "Does regional variation in wage levels identify the effects of a\n  national minimum wage?",
      "author": "Daniel Haanwinckel",
      "coauthors": [],
      "abstract": "This paper investigates the validity of estimators that exploit regional\ndifferences in wage levels to identify the labor market effects of a national\nminimum wage. Specifically, it examines variations of the ``fraction affected''\nand ``effective minimum wage'' designs. The study finds that these estimators\nare prone to biases from correlated measurement errors and functional form\nmisspecification, even when identification assumptions from previous literature\nare met. Additionally, minor deviations from these assumptions can introduce\nsignificant biases. Through a series of simulation exercises and a detailed\ncase study of Brazil's federal minimum wage increase starting in 1995, the\npaper documents the practical relevance of these biases and evaluates the\neffectiveness of potential solutions and diagnostic tools."
    },
    {
      "id": "2411.12036v2",
      "title": "Prediction-Guided Active Experiments",
      "author": "Ruicheng Ao",
      "coauthors": [
        "Hongyu Chen",
        "David Simchi-Levi"
      ],
      "abstract": "In this work, we introduce a new framework for active experimentation, the\nPrediction-Guided Active Experiment (PGAE), which leverages predictions from an\nexisting machine learning model to guide sampling and experimentation.\nSpecifically, at each time step, an experimental unit is sampled according to a\ndesignated sampling distribution, and the actual outcome is observed based on\nan experimental probability. Otherwise, only a prediction for the outcome is\navailable. We begin by analyzing the non-adaptive case, where full information\non the joint distribution of the predictor and the actual outcome is assumed.\nFor this scenario, we derive an optimal experimentation strategy by minimizing\nthe semi-parametric efficiency bound for the class of regular estimators. We\nthen introduce an estimator that meets this efficiency bound, achieving\nasymptotic optimality. Next, we move to the adaptive case, where the predictor\nis continuously updated with newly sampled data. We show that the adaptive\nversion of the estimator remains efficient and attains the same semi-parametric\nbound under certain regularity assumptions. Finally, we validate PGAE's\nperformance through simulations and a semi-synthetic experiment using data from\nthe US Census Bureau. The results underscore the PGAE framework's effectiveness\nand superiority compared to other existing methods."
    },
    {
      "id": "2409.08354v2",
      "title": "Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time\n  Series",
      "author": "Wei Zhang",
      "coauthors": [],
      "abstract": "High-dimensional matrix-valued time series are of significant interest in\neconomics and finance, with prominent examples including cross region\nmacroeconomic panels and firms' financial data panels. We introduce a class of\nBayesian matrix dynamic factor models that utilize matrix structures to\nidentify more interpretable factor patterns and factor impacts. Our model\naccommodates time-varying volatility, adjusts for outliers, and allows\ncross-sectional correlations in the idiosyncratic components. To determine the\ndimension of the factor matrix, we employ an importance-sampling estimator\nbased on the cross-entropy method to estimate marginal likelihoods. Through a\nseries of Monte Carlo experiments, we show the properties of the factor\nestimators and the performance of the marginal likelihood estimator in\ncorrectly identifying the true dimensions of the factor matrices. Applying our\nmodel to a macroeconomic dataset and a financial dataset, we demonstrate its\nability in unveiling interesting features within matrix-valued time series."
    },
    {
      "id": "2411.13372v1",
      "title": "Clustering with Potential Multidimensionality: Inference and Practice",
      "author": "Ruonan Xu",
      "coauthors": [
        "Luther Yap"
      ],
      "abstract": "We show how clustering standard errors in one or more dimensions can be\njustified in M-estimation when there is sampling or assignment uncertainty.\nSince existing procedures for variance estimation are either conservative or\ninvalid, we propose a variance estimator that refines a conservative procedure\nand remains valid. We then interpret environments where clustering is\nfrequently employed in empirical work from our design-based perspective and\nprovide insights on their estimands and inference procedures."
    },
    {
      "id": "2404.13986v2",
      "title": "Stochastic Volatility in Mean: Efficient Analysis by a Generalized\n  Mixture Sampler",
      "author": "Daichi Hiraki",
      "coauthors": [
        "Siddhartha Chib",
        "Yasuhiro Omori"
      ],
      "abstract": "In this paper we consider the simulation-based Bayesian analysis of\nstochastic volatility in mean (SVM) models. Extending the highly efficient\nMarkov chain Monte Carlo mixture sampler for the SV model proposed in Kim et\nal. (1998) and Omori et al. (2007), we develop an accurate approximation of the\nnon-central chi-squared distribution as a mixture of thirty normal\ndistributions. Under this mixture representation, we sample the parameters and\nlatent volatilities in one block. We also detail a correction of the small\napproximation error by using additional Metropolis-Hastings steps. The proposed\nmethod is extended to the SVM model with leverage. The methodology and models\nare applied to excess holding yields and S&P500 returns in empirical studies,\nand the SVM models are shown to outperform other volatility models based on\nmarginal likelihoods."
    },
    {
      "id": "2411.13293v1",
      "title": "Revealed Information",
      "author": "Laura Doval",
      "coauthors": [
        "Ran Eilat",
        "Tianhao Liu",
        "Yangfan Zhou"
      ],
      "abstract": "An analyst observes the frequency with which a decision maker (DM) takes\nactions, but does not observe the frequency of actions conditional on the\npayoff-relevant state. We ask when can the analyst rationalize the DM's choices\nas if the DM first learns something about the state before taking action. We\nprovide a support function characterization of the triples of utility\nfunctions, prior beliefs, and (marginal) distributions over actions such that\nthe DM's action distribution is consistent with information given the agent's\nprior and utility function. Assumptions on the cardinality of the state space\nand the utility function allow us to refine this characterization, obtaining a\nsharp system of finitely many inequalities the utility function, prior, and\naction distribution must satisfy. We apply our characterization to study\ncomparative statics and ring-network games, and to identify conditions under\nwhich a data set is consistent with a public information structure in\nfirst-order Bayesian persuasion games. We characterize the set of distributions\nover posterior beliefs that are consistent with the DM's choices. Assuming the\nfirst-order approach applies, we extend our results to settings with a\ncontinuum of actions and/or states.%"
    },
    {
      "id": "2404.00221v4",
      "title": "Robust Learning for Optimal Dynamic Treatment Regimes with Observational\n  Data",
      "author": "Shosei Sakaguchi",
      "coauthors": [],
      "abstract": "Public policies and medical interventions often involve dynamics in their\ntreatment assignments, where individuals receive a series of interventions over\nmultiple stages. We study the statistical learning of optimal dynamic treatment\nregimes (DTRs) that guide the optimal treatment assignment for each individual\nat each stage based on the individual's evolving history. We propose a doubly\nrobust, classification-based approach to learning the optimal DTR using\nobservational data under the assumption of sequential ignorability. This\napproach learns the optimal DTR through backward induction. At each step, it\nconstructs an augmented inverse probability weighting (AIPW) estimator of the\npolicy value function and maximizes it to learn the optimal policy for the\ncorresponding stage. We show that the resulting DTR can achieve an optimal\nconvergence rate of $n^{-1/2}$ for welfare regret under mild convergence\nconditions on estimators of the nuisance components."
    },
    {
      "id": "2304.08184v4",
      "title": "Adjustment with Many Regressors Under Covariate-Adaptive Randomizations",
      "author": "Liang Jiang",
      "coauthors": [
        "Liyao Li",
        "Ke Miao",
        "Yichong Zhang"
      ],
      "abstract": "Our paper discovers a new trade-off of using regression adjustments (RAs) in\ncausal inference under covariate-adaptive randomizations (CARs). On one hand,\nRAs can improve the efficiency of causal estimators by incorporating\ninformation from covariates that are not used in the randomization. On the\nother hand, RAs can degrade estimation efficiency due to their estimation\nerrors, which are not asymptotically negligible when the number of regressors\nis of the same order as the sample size. Ignoring the estimation errors of RAs\nmay result in serious over-rejection of causal inference under the null\nhypothesis. To address the issue, we construct a new ATE estimator by optimally\nlinearly combining the estimators with and without RAs. We then develop a\nunified inference theory for this estimator under CARs. It has two features:\n(1) the Wald test based on it achieves the exact asymptotic size under the null\nhypothesis, regardless of whether the number of covariates is fixed or diverges\nno faster than the sample size; and (2) it guarantees weak efficiency\nimprovement over estimators both with and without RAs."
    },
    {
      "id": "2409.10549v3",
      "title": "Confronting Conflicts to Yes: Untangling Wicked Problems with Open\n  Design Systems",
      "author": "L. G. Teuber",
      "coauthors": [
        "A. R. M. Wolfert"
      ],
      "abstract": "Current project development practices often fail to engage stakeholders early\nand effectively. Decision support is often non-inclusive, single-sided, and\nlacking in transparency, while complexity goes beyond human's comprehension.\nAdditionally, many approaches focus primarily on technical system aspects,\nneglecting the integration of stakeholders' individual preferences. This often\nresults in project impasses, leaving stakeholders unable to collaboratively\nachieve a \"yes.\" There is a need for a purely associative, a-priori design\napproach that integrates system realities and stakeholder ideals within a joint\nsocio-technical solution space. The state-of-the-art Preferendus, embedded in\nthe proven Open Design Systems (Odesys) methodology, is a neutral tool for\ntransforming complexity into success. Aiming for synthesis, Odesys' robust IMAP\noptimization method generates a single best-fit design solution. Here, Odesys\nis applied for a Dutch wind farm stalemate development, balancing multiple\nstakeholder preferences, wind farm performances, and project constraints. The\nsuccess of this approach hinges on stakeholder trust and input. This article\nintroduces a structured stakeholder assessment method using choice-based\nconjunctive analysis (CBCA), facilitating transparent determination of global\nand local stakeholder weights and preference functions. Modelling 'disputable'\nexogenous factors as endogenous design parameters, the application demonstrates\nhow one can shift toward a collaborative \"yes.\" For this, it is concluded that\na zoomed-out solution space would enable the energy transition to be tackled\nwith multiple options rather than a prescribed one. The Odesys approach fosters\ndecision-making that aligns with the social threefold principles of freedom,\nequality, and fraternity, guiding projects toward genuine democratic outcomes\nrather than selecting from curated options."
    },
    {
      "id": "2411.14301v1",
      "title": "Sustainability concepts for digital research infrastructures developed\n  through ground-level stakeholder empowerment",
      "author": "Florian Ahrens",
      "coauthors": [
        "Dawn Geatches",
        "Niall McCarroll",
        "Justin Buck",
        "Alvaro Lorenzo-Lopez",
        "Hossein Keshtkar",
        "Nadine Fayyad",
        "Hamidreza Hassanloo",
        "Danae Manika"
      ],
      "abstract": "The UK Research and Innovation Digital Research Infrastructure (DRI) needs to\noperate sustainably in the future, encompassing its use of energy and\nresources, and embedded computer hardware carbon emissions. Transition concepts\ntowards less unsustainable operations will inform the future design and\noperations of DRI. A problem remains that, while the skills and knowledge for\nsolving net zero challenges already exist within the UK's DRI community, the\nmechanisms for sharing them and enabling behavior change are missing. Without\nadopting community-driven approaches, individual stakeholders may feel isolated\nand uncertain about how to play their role in the transition. A research\nprogramme was funded to give voice to the ground-level stakeholders of the DRI\necosystem for the co-creation of carbon downshift concepts. This article\npresents the results of the programme, with the goal to inform a fair and just\ntransition from the ground-level, complementing the top-down interventions of\nenergy efficiency policies and renewable energies integration. A workshop-based\ninnovation method was developed for researching stakeholder recommendations and\nperspectives on the sustainable transition of the UK's DRI. We find that giving\na purposeful voice to the stakeholders for shaping their own future sustainable\nDRI environment can be achieved by a guided, expert-integrated, interactive and\nproblem-focused workshop series. The chosen workshop design is impactful on\ncreating bottom-up agency for climate action by first defining the high-level\nproblems of unsustainability in energy and fossil-fuel consumption, and then\nconnecting them to the ground-level circumstances of DRI stakeholders. This\napproach to stakeholder management should initiate a sustainable transition\nthat promises to kick-start impactful changes from within communities, adding\nto high-level efforts from economics, policy, and governance."
    },
    {
      "id": "2411.14275v1",
      "title": "Exploring the Impact of Quizzes Interleaved with Write-Code Tasks in\n  Elementary-Level Visual Programming",
      "author": "Ahana Ghosh",
      "coauthors": [
        "Liina Malva",
        "Alkis Gotovos",
        "Danial Hooshyar",
        "Adish Singla"
      ],
      "abstract": "We explore the role of quizzes in elementary visual programming domains\npopularly used for K-8 computing education. Prior work has studied various quiz\ntypes, such as fill-in-the-gap write-code questions. However, the overall\nimpact of these quizzes is unclear: studies often show utility in the learning\nphase when enhanced with quizzes, though limited transfer of utility in the\npost-learning phase. In this paper, we aim to better understand the impact of\ndifferent quiz types and whether quizzes focusing on diverse skills (e.g., code\ndebugging and task design) would have higher utility. We design a study with\nHour of Code: Maze Challenge by code.org as the base curriculum, interleaved\nwith different quiz types. Specifically, we examine two learning groups: (i)\nHoC-ACE with diverse quizzes including solution tracing, code debugging, code\nequivalence, and task design; (ii) HoC-Fill with simple quizzes on solution\nfinding. We conducted a large-scale study with 405 students in grades 6--7. Our\nresults highlight that the curriculum enhanced with richer quizzes led to\nhigher utility during the post-learning phase."
    },
    {
      "id": "2411.14230v1",
      "title": "Public sentiments on the fourth industrial revolution: An unsolicited\n  public opinion poll from Twitter",
      "author": "Diletta Abbonato",
      "coauthors": [],
      "abstract": "This article explores public perceptions on the Fourth Industrial Revolution\n(4IR) through an analysis of social media discourse across six European\ncountries. Using sentiment analysis and machine learning techniques on a\ndataset of tweets and media articles, we assess how the public reacts to the\nintegration of technologies such as artificial intelligence, robotics, and\nblockchain into society. The results highlight a significant polarization of\nopinions, with a shift from neutral to more definitive stances either embracing\nor resisting technological impacts. Positive sentiments are often associated\nwith technological enhancements in quality of life and economic opportunities,\nwhereas concerns focus on issues of privacy, data security, and ethical\nimplications. This polarization underscores the need for policymakers to engage\nproactively with the public to address fears and harness the benefits of 4IR\ntechnologies. The findings also advocate for digital literacy and public\nawareness programs to mitigate misinformation and foster an informed public\ndiscourse on future technological integration. This study contributes to the\nongoing debate on aligning technological advances with societal values and\nneeds, emphasizing the role of informed public opinion in shaping effective\npolicy."
    },
    {
      "id": "2402.14177v3",
      "title": "Investigating Human Values in Online Communities",
      "author": "Nadav Borenstein",
      "coauthors": [
        "Arnav Arora",
        "Lucie-Aim\u00e9e Kaffee",
        "Isabelle Augenstein"
      ],
      "abstract": "Studying human values is instrumental for cross-cultural research, enabling a\nbetter understanding of preferences and behaviour of society at large and\ncommunities therein. To study the dynamics of communities online, we propose a\nmethod to computationally analyse values present on Reddit. Our method allows\nanalysis at scale, complementing survey based approaches. We train a value\nrelevance and a value polarity classifier, which we thoroughly evaluate using\nin-domain and out-of-domain human annotations. Using these, we automatically\nannotate over six million posts across 12k subreddits with Schwartz values. Our\nanalysis unveils both previously recorded and novel insights into the values\nprevalent within various online communities. For instance, we discover a very\nnegative stance towards conformity in the Vegan and AbolishTheMonarchy\nsubreddits. Additionally, our study of geographically specific subreddits\nhighlights the correlation between traditional values and conservative U.S.\nstates. Through our work, we demonstrate how our dataset and method can be used\nas a complementary tool for qualitative study of online communication."
    },
    {
      "id": "2411.01940v2",
      "title": "Systematic Mapping Study on Requirements Engineering for Regulatory\n  Compliance of Software Systems",
      "author": "Oleksandr Kosenkov",
      "coauthors": [
        "Parisa Elahidoost",
        "Tony Gorschek",
        "Jannik Fischbach",
        "Daniel Mendez",
        "Michael Unterkalmsteiner",
        "Davide Fucci",
        "Rahul Mohanani"
      ],
      "abstract": "Context: As the diversity and complexity of regulations affecting\nSoftware-Intensive Products and Services (SIPS) is increasing, software\nengineers need to address the growing regulatory scrutiny. As with any other\nnon-negotiable requirements, SIPS compliance should be addressed early in SIPS\nengineering - i.e., during requirements engineering (RE). Objectives: In the\nconditions of the expanding regulatory landscape, existing research offers\nscattered insights into regulatory compliance of SIPS. This study addresses the\npressing need for a structured overview of the state of the art in software RE\nand its contribution to regulatory compliance of SIPS. Method: We conducted a\nsystematic mapping study to provide an overview of the current state of\nresearch regarding challenges, principles and practices for regulatory\ncompliance of SIPS related to RE. We focused on the role of RE and its\ncontribution to other SIPS lifecycle phases. We retrieved 6914 studies\npublished from 2017 until 2023 from four academic databases, which we filtered\ndown to 280 relevant primary studies. Results: We identified and categorized\nthe RE-related challenges in regulatory compliance of SIPS and their potential\nconnection to six types of principles and practices. We found that about 13.6%\nof the primary studies considered the involvement of both software engineers\nand legal experts. About 20.7% of primary studies considered RE in connection\nto other process areas. Most primary studies focused on a few popular\nregulation fields and application domains. Our results suggest that there can\nbe differences in terms of challenges and involvement of stakeholders across\ndifferent fields of regulation. Conclusion: Our findings highlight the need for\nan in-depth investigation of stakeholders' roles, relationships between process\nareas, and specific challenges for distinct regulatory fields to guide research\nand practice."
    },
    {
      "id": "2409.16098v2",
      "title": "The Digital Transformation in Health: How AI Can Improve the Performance\n  of Health Systems",
      "author": "\u00c1frica Peri\u00e1\u00f1ez",
      "coauthors": [
        "Ana Fern\u00e1ndez del R\u00edo",
        "Ivan Nazarov",
        "Enric Jan\u00e9",
        "Moiz Hassan",
        "Aditya Rastogi",
        "Dexian Tang"
      ],
      "abstract": "Mobile health has the potential to revolutionize health care delivery and\npatient engagement. In this work, we discuss how integrating Artificial\nIntelligence into digital health applications-focused on supply chain, patient\nmanagement, and capacity building, among other use cases-can improve the health\nsystem and public health performance. We present an Artificial Intelligence and\nReinforcement Learning platform that allows the delivery of adaptive\ninterventions whose impact can be optimized through experimentation and\nreal-time monitoring. The system can integrate multiple data sources and\ndigital health applications. The flexibility of this platform to connect to\nvarious mobile health applications and digital devices and send personalized\nrecommendations based on past data and predictions can significantly improve\nthe impact of digital tools on health system outcomes. The potential for\nresource-poor settings, where the impact of this approach on health outcomes\ncould be more decisive, is discussed specifically. This framework is, however,\nsimilarly applicable to improving efficiency in health systems where scarcity\nis not an issue."
    }
  ]
}